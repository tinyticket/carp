{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "import numpy as np\n",
    "import os\n",
    "import pulp\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "from astral import LocationInfo\n",
    "from astral.sun import sun\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import netCDF4 as nc\n",
    "import tqdm\n",
    "from calendar import monthrange\n",
    "\n",
    "#create dummy expansion file with one row years from 2021 to 2030 and the other column expansion starting from -14000000 in 2021 and going up in 7000000 steps\n",
    "# Create a DataFrame from 2023 to 2050 with each year increasing by 10000 in expansion\n",
    "data = {\n",
    "    \"Year\": [2020,2021,2022,2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040,2041,2042,2043,2044,2045,2046,2047,2048,2049,2050,2051,2052,2053,2054,2055,2056,2057,2058,2059,2060,2061,2062,2063,2064,2065,2066,2067,2068,2069,2070,2071,2072,2073,2074,2075],\n",
    "    \"PV Expansion\": [0.0,0.0,0.0,0.0, 13.9, 20.0, 20.0, 22.0, 22.0, 21.5, 21.5, 18.8, 18.8, 18.8, 18.8, 18.8, 18.2, 18.2, 18.2, 18.2, 18.2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "}\n",
    "expansion_df = pd.DataFrame({\n",
    "    \"Year\": data[\"Year\"],\n",
    "    \"PV Expansion\": data[\"PV Expansion\"]\n",
    "})\n",
    "expansion_df['PV Expansion'] = expansion_df['PV Expansion']*1000000 #convert to kW\n",
    "plz_shapefile=gpd.read_file('georef-germany-postleitzahl.shp')\n",
    "plz_shapefile=plz_shapefile.to_crs('EPSG:4326')\n",
    "#make plz_shapefile a df\n",
    "plz_df=pd.DataFrame(plz_shapefile)\n",
    "\n",
    "# sdr=xr.open_dataset('get_data/PV-Input/rsds_EUR-11_MPI-M-MPI-ESM-LR_rcp26_r1i1p1_SMHI-RCA4_v1a_mon_202101-203012.nc') \n",
    "# ws=xr.open_dataset('get_data/PV-Input/sfcWind_EUR-11_MPI-M-MPI-ESM-LR_rcp26_r1i1p1_SMHI-RCA4_v1a_mon_202101-203012.nc')\n",
    "# #TODO: Maxtemp\n",
    "# at=xr.open_dataset('get_data/PV-Input/tasmax_EUR-11_MPI-M-MPI-ESM-LR_rcp26_r1i1p1_SMHI-RCA4_v1a_mon_202101-203012.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_shapefiles_with_PV_data_optimized(plz_shapefile, sdr, varname):\n",
    "           # Read the PLZ shapefile\n",
    "        plz_gdf = plz_shapefile.copy()\n",
    "        #adjust the crs to a metric crs as this will help the buffer\n",
    "        sdr.crs = 'epsg:4647'\n",
    "\n",
    "        plz_gdf.crs='epsg:4647'\n",
    "\n",
    "        \n",
    "\n",
    "        # Define time columns and initialize new columns in plz_gdf\n",
    "        time_cols = [col for col in sdr.columns if col not in ['geometry']]\n",
    "        for time_col in time_cols:\n",
    "            plz_gdf[f'{varname}{time_col}'] = None\n",
    "        plz_gdf['point_ids'] = None\n",
    "        plz_gdf['point_count'] = 0\n",
    "        plz_gdf['type'] = None\n",
    "        plz_gdf['closest_point_distance'] = None\n",
    "\n",
    "        # Spatial join for points within polygons\n",
    "        joined_gdf = gpd.sjoin(plz_gdf, sdr, how='left', op='contains')\n",
    "        for idx, group in joined_gdf.groupby(joined_gdf.index):\n",
    "            data_pts = group.dropna(subset=['ID'])\n",
    "            plz_gdf.at[idx, 'point_ids'] = list(data_pts['ID'].astype(int))\n",
    "            plz_gdf.at[idx, 'point_count'] = len(data_pts)\n",
    "            plz_gdf.at[idx, 'type'] = 'within' if len(data_pts) > 0 else None\n",
    "            for time_col in time_cols:\n",
    "                plz_gdf.at[idx, f'{varname}{time_col}'] = data_pts[time_col].mean()\n",
    "\n",
    "        # Optimized handling of PLZ polygons without wind points\n",
    "        no_value_plz = plz_gdf[plz_gdf['point_count'] == 0]\n",
    "        buffer_distance = 14000  # 14 km buffer\n",
    "        sindex = sdr.sindex  # Spatial index for sdr\n",
    "\n",
    "        for idx, row in no_value_plz.iterrows():\n",
    "            # Use spatial index to narrow down the candidates\n",
    "            potential_matches_index = list(sindex.intersection(row.geometry.buffer(buffer_distance).bounds))\n",
    "            potential_matches = sdr.iloc[potential_matches_index]\n",
    "            # Calculate the nearest point\n",
    "            nearest_point_data = potential_matches.distance(row.geometry).idxmin()\n",
    "            nearest_point = sdr.loc[nearest_point_data] if pd.notna(nearest_point_data) else None\n",
    "\n",
    "            if nearest_point is not None:\n",
    "                nearest_point_id = nearest_point.name\n",
    "                plz_gdf.at[idx, 'point_ids'] = [nearest_point_id]\n",
    "                plz_gdf.at[idx, 'point_count'] = 1\n",
    "                plz_gdf.at[idx, 'type'] = 'nearby'\n",
    "                plz_gdf.at[idx, 'closest_point_distance'] = row.geometry.distance(nearest_point.geometry)\n",
    "                for time_col in time_cols:\n",
    "                   plz_gdf.loc[idx, f\"{varname}{time_col}\"] = nearest_point[time_col]\n",
    "        return plz_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_germany_gdf_from_ds(ds, varname, lat_min=46, lat_max=56, lon_min=5, lon_max=16):\n",
    "    \"\"\"\n",
    "    Creates a GeoDataFrame containing points within Germany's geographic bounds from a given dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - ds: The dataset containing latitude, longitude, and wind speed data.\n",
    "    - lat_min, lat_max, lon_min, lon_max: Geographic bounds for filtering the points.\n",
    "    \n",
    "    Returns:\n",
    "    - A GeoDataFrame with points within the specified bounds and wind speed data for each time step.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame from dataset coordinates\n",
    "    lat_lon_df = pd.DataFrame({\n",
    "        'lat': ds['lat'].values.ravel(),\n",
    "        'lon': ds['lon'].values.ravel()\n",
    "    })\n",
    "\n",
    "    # Create a GeoDataFrame from lat_lon_df\n",
    "    gdf = gpd.GeoDataFrame(lat_lon_df, geometry=gpd.points_from_xy(lat_lon_df.lon, lat_lon_df.lat))\n",
    "\n",
    "    # Add each time step as a new column with simplified name 'year-month'\n",
    "    for time in ds['time'].values:\n",
    "        simplified_time_name = pd.to_datetime(time).strftime('%Y-%m')\n",
    "        data_slice = ds.sel(time=time)\n",
    "        gdf[simplified_time_name] = data_slice[varname].values.ravel()\n",
    "\n",
    "    # Filter the GeoDataFrame for points within the specified geographic bounds\n",
    "    germany_gdf = gdf[(gdf['lat'] >= lat_min) & (gdf['lat'] <= lat_max) & (gdf['lon'] >= lon_min) & (gdf['lon'] <= lon_max)]\n",
    "    germany_gdf['ID'] = germany_gdf.index\n",
    "    #use only lat lon and geometry and the columns starting with year_oi\n",
    "    #germany_gdf = germany_gdf[['lat', 'lon', 'geometry', 'ID'] + [col for col in germany_gdf.columns if col.startswith('20')]]\n",
    "    #\n",
    "    return germany_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_year_for_germany(germany_gdf_template, varname):\n",
    "    results = []\n",
    "    \n",
    "\n",
    "    ds = pd.read_csv('Data/pv_interim/Germany_gdf_AT_RCP26.csv')\n",
    "    \n",
    "    # Create a base DataFrame from the coordinates (if not already created)\n",
    "    if germany_gdf_template is None:\n",
    "        lat_lon_df = pd.DataFrame({\n",
    "            'lat': ds['lat'].values.ravel(),\n",
    "            'lon': ds['lon'].values.ravel()\n",
    "        })\n",
    "        germany_gdf = gpd.GeoDataFrame(lat_lon_df, geometry=gpd.points_from_xy(lat_lon_df.lon, lat_lon_df.lat))\n",
    "    else:\n",
    "        germany_gdf = germany_gdf_template.copy()\n",
    "    \n",
    "    # Extract all years data for 'sfcWind'\n",
    "    for time in ds['time'].values:\n",
    "        simplified_time_name = pd.to_datetime(time).strftime('%Y-%m')\n",
    "        data_slice = ds.sel(time=time)\n",
    "        germany_gdf[simplified_time_name] = data_slice[varname].values.ravel()\n",
    "    \n",
    "    # Add dataset identifier based on the path (to distinguish between RCP scenarios)\n",
    "    rcp_id = path.split('_')[3]  # Assumes RCP info is the 4th element in the filename\n",
    "    germany_gdf['RCP'] = rcp_id\n",
    "    \n",
    "    results.append(germany_gdf)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_id_to_plz_mapping(germany_gdf):\n",
    "    # This assumes you have a function to get the nearest ID as previously discussed\n",
    "    # Assuming process_shapefiles_with_PV_data_optimized returns a DataFrame with 'nearest_ID'\n",
    "    #plz_gdf = process_shapefiles_with_PV_data_optimized(plz_shapefile, germany_gdf, varname=\"your_varname\")\n",
    "\n",
    "    # Create a dictionary to map ID to PLZ and LAN codes\n",
    "    mapping_dict = germany_gdf.drop_duplicates(subset='plz_code').set_index('plz_code')[['lan_code', 'ID']].to_dict('index')\n",
    "\n",
    "    return mapping_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the mapping DataFrame with the detailed data DataFrame\n",
    "# Convert dictionary to DataFrame for easy merging\n",
    "def map_data_to_plz(germany_gdf, mapping_dict, varname, year_oi):\n",
    "    # Create DataFrame from mapping dictionary and rename 'index' to 'plz_code'\n",
    "    plz_mapping_df = pd.DataFrame.from_dict(mapping_dict, orient='index').reset_index().rename(columns={'index': 'plz_code'})\n",
    "\n",
    "    #make ID col string\n",
    "    plz_mapping_df['ID'] = plz_mapping_df['ID'].astype(int)\n",
    "    germany_gdf['ID'] = germany_gdf['ID'].astype(int)\n",
    "    # Merge with germany_gdf to get additional data based on ID\n",
    "    result_df = plz_mapping_df.merge(germany_gdf, on='ID', how='left')\n",
    "\n",
    "    # Save the resulting DataFrame to a CSV file\n",
    "    result_df.to_csv(f'Data/pv_interim/Germany_gdf_{varname}_{year_oi}.csv', index=False)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mappings(germany_gdf, id_to_plz_mapping):\n",
    "    # Assume germany_gdf has an 'ID' column which matches keys in id_to_plz_mapping\n",
    "    germany_gdf['plz_code'] = germany_gdf['ID'].apply(lambda x: id_to_plz_mapping[x]['plz_code'] if x in id_to_plz_mapping else None)\n",
    "    germany_gdf['lan_code'] = germany_gdf['ID'].apply(lambda x: id_to_plz_mapping[x]['lan_code'] if x in id_to_plz_mapping else None)\n",
    "\n",
    "    return germany_gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_year(germany_gdf,plz_shapefile,varname,scenario):\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    #####für test ds: nur die ersten 100 zeilen von plz\n",
    "    #plz=plz.head(100)\n",
    "\n",
    "\n",
    "\n",
    "    germany_gdf = germany_gdf[['ID','lat', 'lon','geometry'] + [col for col in germany_gdf.columns if col.startswith('20')]]\n",
    "    germany_gdf = gpd.GeoDataFrame(germany_gdf, geometry='geometry')\n",
    "\n",
    "\n",
    "    # %%\n",
    "    updated_plz_gdf = process_shapefiles_with_PV_data_optimized(plz, germany_gdf,varname)\n",
    "\n",
    "    print(\"updated_plz_gdf erzeugt\")\n",
    "\n",
    "    # %%\n",
    "    #drop unnecessary columns name, plz_name, plz_name_lo, krs_code, lan_name, lan_code, krs_name, plz_int\n",
    "    #updated_plz_gdf.drop(['name', 'plz_name', 'plz_name_lo', 'krs_code', 'lan_name', 'krs_name', 'PLZ_int'], axis=1, inplace=True)\n",
    "    #updated_plz_gdf.drop(['point_count','point_ids','closest_point_distance','type'], axis=1, inplace=True)\n",
    "    #check columns\n",
    "    updated_plz_gdf.columns\n",
    "\n",
    "    # %%\n",
    "\n",
    "    #check if values double in plz_code column\n",
    "    updated_plz_gdf['plz_code'].value_counts()\n",
    "    #delete duplicates\n",
    "    updated_plz_gdf.drop_duplicates(subset='plz_code', keep='first', inplace=True)\n",
    "    \n",
    "    #delete length of varname from column names if startswith varname\n",
    "    updated_plz_gdf.columns = [col[len(varname):] if col.startswith(varname) else col for col in updated_plz_gdf.columns]\n",
    "    #save as csv\n",
    "    updated_plz_gdf.to_csv(f'Data/pv_interim/Germany_gdf_{varname}_{scenario}.csv')\n",
    "    return updated_plz_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def kelvin_to_celsius(germany_gdfAT, varname, year_oi, scenario):\n",
    "    year_oistr=str(year_oi)\n",
    "    scenario_str=str(scenario)  \n",
    "    #convert germany_gdf to °C\n",
    "    cols_to_convert = [col for col in germany_gdfAT if col.startswith('20')]\n",
    "\n",
    "    # Apply the conversion formula to each of these columns\n",
    "    for col in cols_to_convert:\n",
    "        germany_gdfAT[col] = germany_gdfAT[col] - 273.15\n",
    "    germany_gdfAT.to_csv(f'Data/pv_interim/Germany_gdf_{varname}_{scenario_str}_{year_oistr}.csv')\n",
    "    return germany_gdfAT\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def avg_daily_sunhours(year_oi,scenario):\n",
    "    year_oistr=str(year_oi)\n",
    "    germany_gdfSDR = pd.read_csv(f'Data/pv_interim/{scenario}/germany_gdf_SDR_{scenario}_{year_oi}.csv')\n",
    "    timezone = 'Europe/Berlin'\n",
    "    #create empty dataframe to store the results with the columns plz_code, and 01-2024, 02-2024, 03-2024, 04-2024, 05-2024, 06-2024, 07-2024, 08-2024, 09-2024, 10-2024, 11-2024, 12-2024\n",
    "    daylight_df=pd.DataFrame(columns=['plz_code']+['lat']+['lon'] +[year_oistr+'-'+f'{str(i).zfill(2)}' for i in range(1,13)])  \n",
    "\n",
    "\n",
    "    daylight_df['plz_code']=germany_gdfSDR['plz_code']\n",
    "    daylight_df['lat']=germany_gdfSDR['lat']\n",
    "    daylight_df['lon']=germany_gdfSDR['lon']\n",
    "\n",
    "\n",
    "    #delete rows with nan values\n",
    "    daylight_df.dropna(inplace=True)\n",
    "    # Assuming 'daylight_df' is already initialized and contains 'plz_code', 'lat', 'lon', and month columns\n",
    "\n",
    "    for index, row in daylight_df.iterrows():\n",
    "        # Extract latitude and longitude for the current row\n",
    "        lat, lon = row['lat'], row['lon']\n",
    "        \n",
    "        for month_col in [year_oistr+'-'+f'{str(i).zfill(2)}' for i in range(1, 13)]:  # Month columns\n",
    "            # Parse the month and year from the column name\n",
    "            month = int(month_col.split('-')[1])\n",
    "            year = int(month_col.split('-')[0])\n",
    "            \n",
    "            # Set the date to the 15th of each month in year_oi\n",
    "            date = datetime.datetime(year, month, 15)\n",
    "            date = date.replace(tzinfo=pytz.timezone(timezone))\n",
    "            \n",
    "            # Create a LocationInfo object for the location\n",
    "            loc = LocationInfo(latitude=lat, longitude=lon)\n",
    "            \n",
    "            # Calculate solar events (sunrise, sunset) for the date\n",
    "            solar_events = sun(loc.observer, date=date)\n",
    "            \n",
    "            # Calculate the duration of daylight in hours\n",
    "            daylight_duration = (solar_events['sunset'] - solar_events['sunrise']).total_seconds() / 3600\n",
    "            \n",
    "            # Update the DataFrame with the calculated daylight hours\n",
    "            daylight_df.at[index, month_col] = daylight_duration\n",
    "\n",
    "    #save as csv\n",
    "    daylight_df.to_csv(f'Data/pv_interim/daylight_df_{scenario}_{year_oistr}.csv')\n",
    "    # Assuming you might want to see the DataFrame or use it further\n",
    "    return daylight_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import calendar\n",
    "\n",
    "# # Iterate over the columns (months) in daylight_df to calculate total daylight hours\n",
    "# for month_col in [year_oistr+'-'f'{str(i).zfill(2)}' for i in range(1, 13)]:\n",
    "#     # Extract the month number from the column name\n",
    "#     month = int(month_col.split('-')[1])\n",
    "    \n",
    "#     # Get the number of days in the month\n",
    "#     days_in_month = calendar.monthrange(year_oi, month)[1]\n",
    "    \n",
    "#     # Multiply the daylight hours for the 15th by the number of days in the month\n",
    "#     # Update the DataFrame with the total daylight hours for each month and location\n",
    "#     daylight_df_cum=daylight_df.copy()\n",
    "#    # daylight_df_cum[[month_col]] = daylight_df[month_col] * days_in_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_sdr_rates(germany_gdfSDR, daylight_df, year_oi):\n",
    "    \"\"\"\n",
    "    # Since the values of germany_gdfSDR are the monthly average SDR in W/m^2  we have to firstly readjust the values to the daytime radiation, since at night it's virtually zero.\n",
    "    # This can be done by multiplying the monthly average by the daylight hours of the respective month divided by 24 hours.\n",
    "    # This will give us the average radiation for the daylight hours of the respective month.\n",
    "    # Next we will adjust the W/m² values to kWh/m² by multiplying the values by the daylight hours (to have Wh) and then by 0.001 to convert them to kWh/m².\n",
    "    # Iterate through each month column in germany_gdfSDR\n",
    "    Adjust SDR rates based on actual daylight hours and convert to kWh/m².\n",
    "    \n",
    "    Parameters:\n",
    "    - germany_gdfSDR: DataFrame containing SDR rates and location data (plz_code, lat, lon).\n",
    "    - daylight_df: DataFrame containing total daylight hours for each month.\n",
    "    - year_oi: The year of interest.\n",
    "    \n",
    "    Returns:\n",
    "    - adj_SDR_rate: DataFrame with adjusted SDR rates in kWh/m² and location data.\n",
    "    \"\"\"\n",
    "    year_oistr = str(year_oi)\n",
    "    # Generate days in each month for year_oi\n",
    "    days_in_month = {f\"{year_oistr}-{str(i).zfill(2)}\": calendar.monthrange(year_oi, i)[1] for i in range(1, 13)}\n",
    "    \n",
    "    # Create empty dataframe to store the results\n",
    "    adj_SDR_rate = pd.DataFrame(columns=['plz_code', 'lat', 'lon'] + [f\"{year_oistr}-{str(i).zfill(2)}\" for i in range(1,13)])\n",
    "    \n",
    "    # Initialize adj_SDR_rate DataFrame with the structure and data from germany_gdfSDR\n",
    "    adj_SDR_rate['plz_code'] = germany_gdfSDR['plz_code']\n",
    "    adj_SDR_rate['lat'] = germany_gdfSDR['lat']\n",
    "    adj_SDR_rate['lon'] = germany_gdfSDR['lon']\n",
    "    \n",
    "    for month_col in [f\"{year_oistr}-{str(i).zfill(2)}\" for i in range(1, 13)]:\n",
    "        if month_col in germany_gdfSDR.columns and month_col in daylight_df.columns:\n",
    "            # Adjust SDR rates for actual daylight hours and convert to kWh/m²\n",
    "            # Note: Assuming daylight_df contains total daylight hours for each month\n",
    "            adjusted_sdr = germany_gdfSDR[month_col] * (24 / daylight_df[month_col])\n",
    "            \n",
    "            # Store the adjusted SDR rates in adj_SDR_rate DataFrame\n",
    "            adj_SDR_rate[month_col] = adjusted_sdr\n",
    "            \n",
    "            # Convert adjusted SDR from W/m² to kWh/m² for the month\n",
    "            # Assuming the formula should actually apply to adj_SDR_rate instead of modifying germany_gdfSDR again\n",
    "            adj_SDR_rate[month_col] = adjusted_sdr * daylight_df[month_col] * days_in_month[month_col] * 0.001\n",
    "    \n",
    "    return adj_SDR_rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_Tpv(year_oi,scenario):\n",
    "    \"\"\"\n",
    "    Calculates Tpv values for each month based on input DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    - year_oi: Year of interest as a string, e.g., '2024'.\n",
    "    - germany_gdfAT: DataFrame containing AT data with 'plz_code' and monthly values.\n",
    "    - germany_gdfSDR: DataFrame containing SDR data with 'plz_code' and monthly values.\n",
    "    - germany_gdfWS: DataFrame containing WS data with 'plz_code' and monthly values.\n",
    "\n",
    "    Returns:\n",
    "    - Tpv_df: DataFrame containing the calculated Tpv values along with 'plz_code', 'lat', 'lon' for each month.\n",
    "    \"\"\"\n",
    "    year_oistr = str(year_oi)   \n",
    "    germany_gdfAT=pd.read_csv(f'Data/pv_interim/{scenario}/germany_gdf_AT_{scenario}_{year_oi}.csv')\n",
    "    germany_gdfSDR=pd.read_csv(f'Data/pv_interim/{scenario}/germany_gdf_SDR_{scenario}_{year_oi}.csv')\n",
    "    germany_gdfWS=pd.read_csv(f'Data/pv_interim/{scenario}/germany_gdf_WS_{scenario}_{year_oi}.csv')\n",
    "    germany_gdfAT=kelvin_to_celsius(germany_gdfAT, 'AT', year_oi, scenario)\n",
    "    # Create empty DataFrame to store the results\n",
    "    Tpv_df = pd.DataFrame(columns=['plz_code', 'lat', 'lon'] + [f'{year_oi}-{str(i).zfill(2)}' for i in range(1, 13)])\n",
    "    Tpv_df['plz_code'] = germany_gdfSDR['plz_code']\n",
    "    Tpv_df['lat'] = germany_gdfSDR['lat']\n",
    "    Tpv_df['lon'] = germany_gdfSDR['lon']\n",
    "    \n",
    "    # Identify all month columns by excluding non-month columns\n",
    "    month_columns = [col for col in germany_gdfAT.columns if col.startswith(year_oistr)]\n",
    "    \n",
    "    # Initialize Tpv DataFrame with plz_code and NaN for month columns\n",
    "    Tpv = pd.DataFrame({'plz_code': germany_gdfAT['plz_code']})\n",
    "    for month in month_columns:\n",
    "        Tpv[month] = np.nan  # Initialize with NaN\n",
    "    \n",
    "    # Calculate Tpv for each month using the provided formula\n",
    "    for month in month_columns:\n",
    "        Tpv[month] = 2.08 + 1.038 * germany_gdfAT[month] + 0.0182 * germany_gdfSDR[month] - 1.13 * germany_gdfWS[month]\n",
    "    \n",
    "    #save as csv\n",
    "    Tpv.to_csv(f'Data/pv_interim/Tpv_df_{scenario}_{year_oi}.csv',sep=';')\n",
    "    return Tpv\n",
    "# year_oi = '2024'\n",
    "# Tpv_df = calculate_Tpv(year_oi, germany_gdfAT, germany_gdfSDR, germany_gdfWS)\n",
    "# Make sure to replace `germany_gdfAT`, `germany_gdfSDR`, and `germany_gdfWS` with the actual DataFrames containing your data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_annual_contraction(start_year, end_year,typ, input_dir='', output_dir='Data/pv_interim/'):\n",
    "    # Load the combined MASTR data\n",
    "    file_path = f'2023MASTR{typ}.csv'\n",
    "    mastr_data = pd.read_csv(file_path, sep=';')\n",
    "    mastr_data['Inbetriebnahmedatum'] = pd.to_datetime(mastr_data['Inbetriebnahmedatum'])\n",
    "\n",
    "    # Loop through each year to calculate decommissioning for exactly 20-year-old turbines\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        if year != start_year:\n",
    "            # Filter installations that turn exactly 25 years old during the year\n",
    "            filter_year = mastr_data['Inbetriebnahmedatum'].dt.year == (year - 25)\n",
    "            yearly_decommissioned = mastr_data[filter_year]\n",
    "\n",
    "            # Aggregate the decommissioned data by postcode\n",
    "            aggregated_data = yearly_decommissioned.groupby('Postleitzahl').agg(\n",
    "                Total_Nettonennleistung=('Nettonennleistung', 'sum'),\n",
    "                Anzahl_Anlagen=('Postleitzahl', 'count'),\n",
    "\n",
    "                Bundesland=('Bundesland', 'first')\n",
    "            ).reset_index()\n",
    "\n",
    "        else:\n",
    "        # Filter installations that turn exactly 25 years old during the year\n",
    "            filter_year = mastr_data['Inbetriebnahmedatum'].dt.year <= (year - 25)\n",
    "            yearly_decommissioned = mastr_data[filter_year]\n",
    "\n",
    "            # Aggregate the decommissioned data by postcode\n",
    "            aggregated_data = yearly_decommissioned.groupby('Postleitzahl').agg(\n",
    "                Total_Nettonennleistung=('Nettonennleistung', 'sum'),\n",
    "                Anzahl_Anlagen=('Postleitzahl', 'count'),\n",
    "\n",
    "                Bundesland=('Bundesland', 'first')\n",
    "            ).reset_index()\n",
    "        #rename Postleitzahl to plz_code\n",
    "        aggregated_data.rename(columns={'Postleitzahl':'plz_code'}, inplace=True)\n",
    "        \n",
    "        # Save the yearly data\n",
    "        aggregated_data.to_csv(f'{output_dir}/contraction_data_{year}_{typ}.csv', sep=';', index=False)\n",
    "        print(f'Contraction data for year {year} saved, focusing on installations exactly 25 years old.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_active_installations(Typ, year_oi, expansion_df):\n",
    "    \"\"\"\n",
    "    Filters active and deprecated PV installations based on their operational status and age.\n",
    "    Updates the expansion target for the next year by adding the power from deprecated installations,\n",
    "    unless it is the last year in the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - Typ: The type of installation as a string.\n",
    "    - year_oi: The year of interest as an integer.\n",
    "    - expansion_df: DataFrame containing expansion targets for years.\n",
    "    \n",
    "    Returns:\n",
    "    - active_installations: DataFrame of installations not permanently decommissioned and age <= 25 years.\n",
    "    - expansion_df: Updated DataFrame with adjusted expansion values for the next year.\n",
    "    \"\"\"\n",
    "    year_befoistr = str(year_oi - 1)\n",
    "    \n",
    "    mastr_pv = pd.read_csv(f'{year_befoistr}MASTR{Typ}.csv', sep=';')\n",
    "    \n",
    "    # Pre-processing\n",
    "    mastr_pv['Inbetriebnahmedatum'] = pd.to_datetime(mastr_pv['Inbetriebnahmedatum'], format='%Y-%m-%d')\n",
    "    mastr_pv['Age_as_of_year_oi'] = year_oi - mastr_pv['Inbetriebnahmedatum'].dt.year\n",
    "    \n",
    "    # Filter active and deprecated installations\n",
    "    active_installations = mastr_pv[(mastr_pv['EinheitBetriebsstatus'] != 'endgültig stillgelegt') & (mastr_pv['Age_as_of_year_oi'] <= 25)]\n",
    "    deprecated_installations = mastr_pv[(mastr_pv['EinheitBetriebsstatus'] == 'endgültig stillgelegt') | (mastr_pv['Age_as_of_year_oi'] > 25)]\n",
    "    \n",
    "    # Calculate deprecated installations' power\n",
    "    power_deprecated_installations = deprecated_installations['Nettonennleistung'].sum()\n",
    "    \n",
    "    # Check if the current year is the last year in the dataset\n",
    "    if year_oi < 2050:\n",
    "        next_year = year_oi + 1\n",
    "        if next_year in expansion_df['Year'].values:\n",
    "            expansion_df.loc[expansion_df['Year'] == next_year, 'PV Expansion'] += power_deprecated_installations\n",
    "\n",
    "    # Print summary information about deprecated installations\n",
    "    print(f\"Number of deprecated installations in {year_oi}: {len(deprecated_installations)}\")\n",
    "    print(f\"Power of deprecated installations in {year_oi}: {power_deprecated_installations} kWp\")\n",
    "    \n",
    "    return active_installations, expansion_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_on_PLZ_level(Typ, year_oi, plz_df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Aggregates the MaStR data on the PLZ level.\n",
    "    \n",
    "    Parameters:\n",
    "    - mastr_pv: DataFrame containing the MaStR data.\n",
    "    \n",
    "    Returns:\n",
    "    - A DataFrame with aggregated data on the PLZ level.\n",
    "    \"\"\"\n",
    "    # Group by 'Postleitzahl' and aggregate the data\n",
    "    year_befoistr = str(year_oi - 1)\n",
    "    mastr_pv=pd.read_csv(f'{year_oi-1}MASTR{Typ}.csv', sep=';')\n",
    "    # Pre-processing\n",
    "    mastr_pv['Inbetriebnahmedatum'] = pd.to_datetime(mastr_pv['Inbetriebnahmedatum'], format='%Y-%m-%d')\n",
    "    mastr_pv['Age_as_of_year_oi'] = year_oi - mastr_pv['Inbetriebnahmedatum'].dt.year\n",
    "\n",
    "    PV_agg = mastr_pv.groupby('Postleitzahl').agg(\n",
    "        BL=('Bundesland', 'first'),\n",
    "        Total_BRUTTOLEISTUNG=('Nettonennleistung', 'sum'),\n",
    "        num_installation=('Postleitzahl', 'count'),\n",
    "        mean_age=('Age_as_of_year_oi', 'mean')\n",
    "        \n",
    "    ).reset_index()\n",
    "    \n",
    "    # Map the 'BL' column to 'BL_code' using the defined mapping\n",
    "    bl_to_bl_code_mapping = {\n",
    "        'Baden-Württemberg': '08', 'Bayern': '09', 'Berlin': '11', 'Brandenburg': '12', \n",
    "        'Bremen': '04', 'Hamburg': '02', 'Hessen': '06', 'Mecklenburg-Vorpommern': '13', \n",
    "        'Niedersachsen': '03', 'Nordrhein-Westfalen': '05', 'Rheinland-Pfalz': '07', \n",
    "        'Saarland': '10', 'Sachsen': '14', 'Sachsen-Anhalt': '15', \n",
    "        'Schleswig-Holstein': '01', 'Thüringen': '16'\n",
    "    }\n",
    "    PV_agg['BL_code'] = PV_agg['BL'].map(bl_to_bl_code_mapping)\n",
    "    \n",
    "    #about 1 MWp per hectare,meaning 1 MWp per 0.01 km².\n",
    "    #PV_agg['Total_BRUTTOLEISTUNG'] = PV_agg['Total_BRUTTOLEISTUNG'] / 1000\n",
    "    PV_agg.rename(columns={'Postleitzahl':'plz_code'}, inplace=True)\n",
    "    \n",
    "\n",
    "    blpath='BL-Area_2023.csv' #since it is only for name purposes and without change and a potential point of further work, the year is hardcoded\n",
    "\n",
    "    BL_Area=pd.read_csv(blpath,sep=';')\n",
    "    #delete nan values in PV_agg BL_code\n",
    "    PV_agg.dropna(subset=['BL_code'], inplace=True)\n",
    "    BL_Area['BL_code'] = BL_Area['BL_code'].astype(int) \n",
    "    PV_agg['BL_code'] = PV_agg['BL_code'].astype(int)\n",
    "\n",
    "    #add every plz_code value from plz_df to PV_agg. lan_code is the value that shoud be put into BL_code. all other values are 0\n",
    "    \n",
    "    # Create a new dataframe from plz_df with the columns you need\n",
    "    new_entries = plz_df[['plz_code', 'lan_code']].copy()\n",
    "\n",
    "    # Rename the 'lan_code' to 'BL_code' to match the PV_agg DataFrame\n",
    "    new_entries.rename(columns={'lan_code': 'BL_code'}, inplace=True)\n",
    "\n",
    "    # Set all other relevant columns to 0\n",
    "    for col in PV_agg.columns.difference(['plz_code', 'BL_code']):\n",
    "        new_entries[col] = 0\n",
    "    new_entries['BL_code'] = new_entries['BL_code'].astype(int)\n",
    "    new_entries['plz_code'] = new_entries['plz_code'].astype(int)\n",
    "    \n",
    "    # Merge the new entries into PV_agg\n",
    "    # This uses an outer join to ensure all values from both dataframes are included\n",
    "    #use only the first entry of each plz_code value\n",
    "    \n",
    "    PV_agg = pd.merge(PV_agg, new_entries, on='plz_code', how='outer', suffixes=('', '_new'))\n",
    "\n",
    "    # Where original BL_code is NaN (meaning it was missing), fill it with BL_code_new values\n",
    "    PV_agg['BL_code'] = PV_agg['BL_code'].fillna(PV_agg['BL_code_new'])\n",
    "\n",
    "    # Drop the temporary BL_code_new column\n",
    "    # Correctly selecting multiple columns from a DataFrame\n",
    "    PV_agg = PV_agg[['plz_code', 'BL', 'Total_BRUTTOLEISTUNG', 'num_installation', 'mean_age','BL_code']]\n",
    "\n",
    "\n",
    "\n",
    "    # Fill NaN values for other columns in PV_agg with 0, since they might be missing in the new entries\n",
    "    PV_agg.fillna(0, inplace=True)\n",
    "\n",
    "    # Optionally convert columns to integer if they must be integer type\n",
    "    # Convert only 'plz_code' and 'BL_code' columns to integers\n",
    "    if 'plz_code' in PV_agg.columns:\n",
    "        PV_agg['plz_code'] = PV_agg['plz_code'].astype(int)\n",
    "    if 'BL_code' in PV_agg.columns:\n",
    "        PV_agg['BL_code'] = PV_agg['BL_code'].astype(int)\n",
    "\n",
    "    #plz_df=plz_df[['plz_code','lan_code']]\n",
    "\n",
    "    # Create a mapping from BL_code to Bundesland using BL_Area\n",
    "    bl_code_to_bundesland_mapping = BL_Area.set_index('BL_code')['Bundesland'].to_dict()\n",
    "\n",
    "    # # Update the 'Bundesland' column in PV_agg using the mapping\n",
    "    PV_agg['Bundesland'] = PV_agg['BL_code'].map(bl_code_to_bundesland_mapping)\n",
    "    \n",
    "    # # Now PV_agg has 'Bundesland' values that match the representations in BL_Area\n",
    "    # PV_agg drop duplicates\n",
    "    PV_agg.drop_duplicates(subset='plz_code', keep='first', inplace=True)\n",
    "    \n",
    "    BL_Area.to_csv(f'Data/pv_interim/BL-Area_{year_oi}.csv',sep=';', index=False)  \n",
    "    PV_agg.to_csv(f'Data/pv_interim/PV_agg_{year_oi}_{Typ}.csv',sep=';', index=False)\n",
    "    return PV_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year=2024\n",
    "# mastr_pv=pd.read_csv(f'Data/pv_interim/Mastr/2023MASTRFreif.csv', sep=';')\n",
    "# contraction_df=pd.read_csv(f'Data/pv_interim/contraction/contraction_data_2024.csv', sep=';')\n",
    "# PV_agg=pd.read_csv(f'Data/pv_interim/PV_agg_2023_Freif.csv', sep=';')\n",
    "# bonsys=pd.read_csv(f'Data/bonsys2023.csv')\n",
    "# expansion_target = expansion_df.loc[expansion_df['Year'] == year, 'PV Expansion'].item()/2 #since it's about half and half for Baul and Freif\n",
    "# quantiles = mastr_pv['Nettonennleistung'].quantile([0.25, 0.5, 0.75]).values #TODO quantiles for PV power\n",
    "\n",
    "# # Convert plz_code in both dataframes to the same type\n",
    "#  # Handle possible non-integer or float-formatted 'plz_code' values\n",
    "#     bonsys['plz_code'] = pd.to_numeric(bonsys['plz_code'], errors='coerce').fillna(0).astype(int)\n",
    "#     PV_agg['plz_code'] = pd.to_numeric(PV_agg['plz_code'], errors='coerce').fillna(0).astype(int)\n",
    "#     contraction_df['plz_code'] = pd.to_numeric(contraction_df['plz_code'], errors='coerce').fillna(0).astype(int)\n",
    "# #add sum of contraction_df Total_Nettonennleistung to expansion_df\n",
    "# expansion_df.loc[expansion_df['Year'] == year+1, 'PV Expansion'] += contraction_df['Total_Nettonennleistung'].sum()\n",
    "# # Create a DataFrame of missing 'plz_code' with default 'diff' values\n",
    "\n",
    "# missing_plz_codes = PV_agg[~PV_agg['plz_code'].isin(bonsys['plz_code'])]['plz_code'].unique()\n",
    "# missing_plz_df = pd.DataFrame({\n",
    "#     'plz_code': missing_plz_codes,\n",
    "#     'building-installations_pv': [0] * len(missing_plz_codes),  # example default values\n",
    "#     'ground-mounted_installations_pv': [0] * len(missing_plz_codes),\n",
    "#     'diff': [0] * len(missing_plz_codes)  # or some neutral value\n",
    "# })\n",
    "# merged_df = pd.merge(PV_agg, contraction_df, on='plz_code', how='left', suffixes=('', '_contraction'))\n",
    "# merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# merged_df['Total_Nettonennleistung'].fillna(0, inplace=True)\n",
    "# merged_df['Anzahl_Anlagen'].fillna(0, inplace=True)\n",
    "# merged_df['Total_BRUTTOLEISTUNG'] -= merged_df['Total_Nettonennleistung']\n",
    "# merged_df['num_installation'] -= merged_df['Anzahl_Anlagen']\n",
    "# # Assumption: New installations have an age of 0 at the time of installation\n",
    "# merged_df['total_old_age'] = merged_df['num_installation'] * merged_df['mean_age']\n",
    "\n",
    "# # New installations have an age of 0, so their total age contribution is 0\n",
    "# # This line is illustrative and can be skipped as it does not change the calculation\n",
    "# # merged_df['total_new_age'] = merged_df['Anzahl_Anlagen'] * 0\n",
    "\n",
    "# # Calculate new mean age\n",
    "# merged_df['new_mean_age'] = (merged_df['total_old_age'] + 0) / (merged_df['num_installation'] + merged_df['Anzahl_Anlagen'])\n",
    "\n",
    "# # Handle divisions by zero if there are no installations at all\n",
    "# merged_df['new_mean_age'] = merged_df['new_mean_age'].fillna(0)\n",
    "\n",
    "# # Optionally, drop the temporary column used for calculation\n",
    "# #merged_df.drop(columns=['total_old_age'], inplace=True)\n",
    "# PV_agg=merged_df[['plz_code', 'BL', 'Total_BRUTTOLEISTUNG', 'num_installation', 'mean_age', 'BL_code', 'Bundesland']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_pv(year, expansion_df, installation_type):\n",
    "    # Load datasets\n",
    "    mastr_pv=pd.read_csv(f'2023MASTR{installation_type}.csv', sep=';')\n",
    "    contraction_df=pd.read_csv(f'Data/pv_interim/contraction_data_{year}_{installation_type}.csv', sep=';')\n",
    "    PV_agg=pd.read_csv(f'PV_agg_{year-1}_{installation_type}.csv', sep=';')\n",
    "    bonsys=pd.read_csv(f'Data/pv_interim/bonsys{year-1}.csv')\n",
    "    expansion_target = expansion_df.loc[expansion_df['Year'] == year, 'PV Expansion'].item()/2 #since it's about half and half for Baul and Freif\n",
    "    \n",
    "    # Convert plz_code in both dataframes to the same type\n",
    "    # Handle possible non-integer or float-formatted 'plz_code' values\n",
    "    bonsys['plz_code'] = pd.to_numeric(bonsys['plz_code'], errors='coerce').fillna(0).astype(int)\n",
    "    PV_agg['plz_code'] = pd.to_numeric(PV_agg['plz_code'], errors='coerce').fillna(0).astype(int)\n",
    "    contraction_df['plz_code'] = pd.to_numeric(contraction_df['plz_code'], errors='coerce').fillna(0).astype(int)\n",
    "    #add sum of contraction_df Total_Nettonennleistung to expansion_df\n",
    "    expansion_df.loc[expansion_df['Year'] == year+1, 'PV Expansion'] += contraction_df['Total_Nettonennleistung'].sum()\n",
    "    # Create a DataFrame of missing 'plz_code' with default 'diff' values\n",
    "    expansion_chunk_value=mastr_pv['Nettonennleistung'].quantile(0.75) \n",
    "     \n",
    "    missing_plz_codes = PV_agg[~PV_agg['plz_code'].isin(bonsys['plz_code'])]['plz_code'].unique()\n",
    "    missing_plz_df = pd.DataFrame({\n",
    "        'plz_code': missing_plz_codes,\n",
    "        'building-installations_pv': [0] * len(missing_plz_codes),  # example default values\n",
    "        'ground-mounted_installations_pv': [0] * len(missing_plz_codes),\n",
    "        'diff': [0] * len(missing_plz_codes)  # or some neutral value\n",
    "    })\n",
    "    merged_df = pd.merge(PV_agg, contraction_df, on='plz_code', how='left', suffixes=('', '_contraction'))\n",
    "    print(merged_df.columns)\n",
    "    merged_df['Total_Nettonennleistung'].fillna(0, inplace=True)\n",
    "    merged_df['Anzahl_Anlagen'].fillna(0, inplace=True)\n",
    "    merged_df['Total_BRUTTOLEISTUNG'] -= merged_df['Total_Nettonennleistung']\n",
    "    merged_df['num_installation'] -= merged_df['Anzahl_Anlagen']\n",
    "    # Assumption: New installations have an age of 0 at the time of installation\n",
    "    merged_df['total_old_age'] = merged_df['num_installation'] * merged_df['mean_age']\n",
    "\n",
    "    # New installations have an age of 0, so their total age contribution is 0\n",
    "    # This line is illustrative and can be skipped as it does not change the calculation\n",
    "    # merged_df['total_new_age'] = merged_df['Anzahl_Anlagen'] * 0\n",
    "\n",
    "    # Calculate new mean age\n",
    "    merged_df['new_mean_age'] = (merged_df['total_old_age'] + 0) / (merged_df['num_installation'] + merged_df['Anzahl_Anlagen'])\n",
    "\n",
    "    # Handle divisions by zero if there are no installations at all\n",
    "    merged_df['new_mean_age'] = merged_df['new_mean_age'].fillna(0)\n",
    "\n",
    "    # Optionally, drop the temporary column used for calculation\n",
    "    #merged_df.drop(columns=['total_old_age'], inplace=True)\n",
    "    PV_agg=merged_df[['plz_code', 'BL', 'Total_BRUTTOLEISTUNG', 'num_installation', 'mean_age', 'BL_code', 'Bundesland']]\n",
    "    #delete duplicates \n",
    "    PV_agg.drop_duplicates(subset='plz_code', keep='first', inplace=True)\n",
    "    bonsys = pd.concat([bonsys, missing_plz_df], ignore_index=True)\n",
    "\n",
    "    # Calculate the expansion target and determine the total chunks available\n",
    "    expansion_target = expansion_df.loc[expansion_df['Year'] == year, 'PV Expansion'].item() / 2\n",
    "    total_chunks = int(expansion_target / expansion_chunk_value)\n",
    "    print(f\"Year: {year}, Installation Type: {installation_type}, Total Chunks Available: {total_chunks}\")\n",
    "    if installation_type == 'Baul':\n",
    "        base_upBound = 250\n",
    "    elif installation_type == 'Freif':\n",
    "        base_upBound = 10\n",
    "\n",
    "    # Set up the linear programming problem\n",
    "    prob = pulp.LpProblem(\"PV_Distribution\", pulp.LpMaximize)\n",
    "\n",
    "    # Define variables for the number of chunks each plz_code can receive\n",
    "    distribution_units = {}\n",
    "    for plz_code in PV_agg['plz_code']:\n",
    "        diff = bonsys[bonsys['plz_code'] == plz_code]['diff'].iloc[0] if not bonsys[bonsys['plz_code'] == plz_code].empty else 0\n",
    "        if installation_type == 'Baul':\n",
    "            base_upBound = 250\n",
    "        elif installation_type == 'Freif':\n",
    "            base_upBound = 5\n",
    "        upper_bound = max(0, base_upBound + base_upBound * diff)\n",
    "        distribution_units[plz_code] = pulp.LpVariable(f\"Chunks_{plz_code}\", lowBound=0, upBound=upper_bound, cat='Integer')\n",
    "\n",
    "    # Objective function to maximize the total power distributed\n",
    "    prob += pulp.lpSum(distribution_units[plz] * expansion_chunk_value for plz in distribution_units)\n",
    "\n",
    "    # Add constraint to ensure the total power does not exceed the available chunks\n",
    "    prob += pulp.lpSum(distribution_units[plz] * expansion_chunk_value for plz in distribution_units) <= expansion_target\n",
    "\n",
    "    # Solve the problem\n",
    "    prob.solve()\n",
    "\n",
    "    # Collect and print the results\n",
    "    if pulp.LpStatus[prob.status] == 'Optimal':\n",
    "        print(\"Optimal solution found.\")\n",
    "        for plz in distribution_units:\n",
    "            units = distribution_units[plz].varValue\n",
    "            if units > 0:\n",
    "                distributed_power = units * expansion_chunk_value\n",
    "                PV_agg.loc[PV_agg['plz_code'] == plz, 'Total_BRUTTOLEISTUNG'] += distributed_power\n",
    "                PV_agg.loc[PV_agg['plz_code'] == plz, 'num_installation'] += units\n",
    "                print(f\"PLZ {plz}: {units} chunks, {distributed_power} kW added\")\n",
    "    else:\n",
    "        print(\"Failed to find an optimal solution.\")\n",
    "\n",
    "    # Update and save data\n",
    "    PV_agg.to_csv(f'Data/pv_interim/PV_agg_{year}_{installation_type}.csv', sep=';', index=False)\n",
    "    bonsys.to_csv(f'Data/pv_interim/bonsys{year}.csv', index=False)\n",
    "    return prob, PV_agg\n",
    "\n",
    "# # Example usage\n",
    "# year = 2024\n",
    "# #expansion_df = pd.DataFrame({'Year': [2024], 'PV Expansion': [20000000]})  # Total expansion for the year\n",
    "# installation_type = 'Baul'\n",
    "# #expansion_chunk_value = 2000  # 1 kW per chunk\n",
    "# prob, PV_agg = distribute_pv(year, expansion_df, installation_type)\n",
    "# print(PV_agg['Total_BRUTTOLEISTUNG'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_mastrs(PV_agg, PV_aggB, mastr_pv, mastr_pvB,year_oi):\n",
    "    \"\"\"\n",
    "    Joins two pairs of DataFrames: one pair for aggregated PV data and another for detailed MaStR data.\n",
    "\n",
    "    Parameters:\n",
    "    - PV_agg: First DataFrame of aggregated PV data.\n",
    "    - PV_aggB: Second DataFrame of aggregated PV data to be joined with the first.\n",
    "    - mastr_pv: First DataFrame of detailed MaStR data.\n",
    "    - mastr_pvB: Second DataFrame of detailed MaStR data to be joined with the first.\n",
    "\n",
    "    Returns:\n",
    "    - A tuple of joined DataFrames: (joined PV_agg, joined mastr_pv).\n",
    "    \"\"\"\n",
    "    year_oistr = str(year_oi)\n",
    "    # Join the two aggregated dataframes using concat for better handling of similar structured data\n",
    "    joined_PV_agg = pd.concat([PV_agg, PV_aggB], ignore_index=True)\n",
    "\n",
    "    # Append the second detailed MaStR data to the first\n",
    "    joined_mastr_pv = pd.concat([mastr_pv, mastr_pvB], ignore_index=True)\n",
    "    joined_mastr_pv.to_csv(f'Data/pv_interim/mastr_pv_{year_oistr}.csv')\n",
    "    #save as csv\n",
    "    joined_PV_agg.to_csv(f'Data/pv_interim/PV_agg_{year_oistr}.csv')\n",
    "    #joined_mastr_pv.to_csv(f'Data/pv_interim/mastr_pv_{year_oistr}.csv')\n",
    "    return joined_PV_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_irradiance_to_PSH(year, scenario):\n",
    "    # Create a copy of the dataframe to avoid modifying the original data\n",
    "    df = pd.read_csv(f'Data/pv_interim/{scenario}/germany_gdf_SDR_{scenario}_{year}.csv')\n",
    "    \n",
    "    # Identify all columns with irradiance data for the specified year\n",
    "    irradiance_columns = [col for col in df.columns if col.startswith(str(year))]\n",
    "\n",
    "    # Process each column\n",
    "    for month_col in irradiance_columns:\n",
    "        # Split to get the month number from the column name, e.g., '2024-01' -> 1\n",
    "        _, month = map(int, month_col.split('-'))\n",
    "        # Calculate the number of hours in the month\n",
    "        hours_in_month = monthrange(year, month)[1] * 24\n",
    "        \n",
    "        # Calculate total kWh/m² for the month\n",
    "        df[f'{month_col}'] = (df[month_col] * hours_in_month) / 1000\n",
    "\n",
    "    #save the results as a csv file\n",
    "    df.to_csv(f'Data/pv_interim/PSH_{year}_{scenario}.csv', index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_actual_power_output(year_oi, scenario, typ):\n",
    "    efficiency = 0.23\n",
    "    aging_factor = 0.007\n",
    "    area_m2_per_kWp = 6\n",
    "    muPmax = 0.0034\n",
    "    T_ref = 25\n",
    "    year_oistr = str(year_oi)\n",
    "    PV_agg = pd.read_csv(f'Data/pv_interim/PV_agg_{year_oistr}_{typ}.csv', sep=';')    \n",
    "    Tpv = pd.read_csv(f'Data/pv_interim/Tpv_df_{scenario}_{year_oistr}.csv', sep=';')\n",
    "    PSH=pd.read_csv(f'Data/pv_interim/PSH_{year_oistr}_{scenario}.csv')\n",
    "    daylight_df = pd.read_csv(f'Data/pv_interim/daylight_df_{scenario}_{year_oistr}.csv') \n",
    "    # Ensure DataFrame 'plz_code' columns are of the same type for proper merging\n",
    "    PV_agg['plz_code'] = PV_agg['plz_code'].astype(int)\n",
    "    Tpv['plz_code'] = Tpv['plz_code'].astype(int)\n",
    "    PSH['plz_code'] = PSH['plz_code'].astype(int)\n",
    "    daylight_df['plz_code'] = daylight_df['plz_code'].astype(int)\n",
    "\n",
    "    # Initialize a list to hold data\n",
    "    results = []\n",
    "\n",
    "    # Iterate over each month\n",
    "    for month in range(1, 13):\n",
    "        month_str = f\"{year_oi}-{str(month).zfill(2)}\"\n",
    "        \n",
    "        if month_str in Tpv.columns and month_str in PSH.columns and month_str in daylight_df.columns:\n",
    "            T_cell = Tpv[month_str]\n",
    "            G = PSH[month_str]\n",
    "            D = daylight_df[month_str]\n",
    "\n",
    "            # Calculate the adjusted efficiencies\n",
    "            eta_age = (1 - aging_factor * PV_agg['mean_age'])\n",
    "            eta_temp = eta_age * (1 - muPmax * (T_cell - T_ref))\n",
    "\n",
    "            # Calculate area and power output incorporating daylight hours fraction\n",
    "            A = PV_agg['Total_BRUTTOLEISTUNG']\n",
    "            power_output = (G * A * eta_temp*0.87 ).clip(lower=0) #13% wechslerverluste\n",
    "\n",
    "            # Append each row to results\n",
    "            for plz_code, production in zip(PV_agg['plz_code'], power_output):\n",
    "                results.append({\n",
    "                    'plz_code': plz_code,\n",
    "                    'year': year_oi,\n",
    "                    'month': month,\n",
    "                    'Production': production\n",
    "                })\n",
    "\n",
    "    # Convert list to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    " \n",
    "    PVsum = results_df['Production'].sum() / 1000000000\n",
    "    print(f'Production in {year_oistr} is {PVsum} TWh for {typ} installations')\n",
    "    # Save to CSV\n",
    "    results_df.to_csv(f'Data/Results/PV_power_output_{scenario}_{year_oi}_{typ}.csv', index=False)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_Power_outputs(year, scenario):\n",
    "    ProdFreif = pd.read_csv(f'Data/Results/PV_power_output_{scenario}_{year}_Freif.csv', sep=',')\n",
    "    ProdBaul = pd.read_csv(f'Data/Results/PV_power_output_{scenario}_{year}_Baul.csv', sep=',') \n",
    "    \n",
    "    \n",
    "    # Merging the dataframes on 'plz_code', 'year', and 'month' with an outer join to keep all plz_code values\n",
    "    merged_df = pd.merge(ProdFreif, ProdBaul, on=['plz_code', 'year', 'month'], how='outer', suffixes=('_ProdFreif', '_ProdBaul'))\n",
    "\n",
    "    # Filling NaN values with 0 for addition\n",
    "    merged_df['Production_ProdFreif'].fillna(0, inplace=True)\n",
    "    merged_df['Production_ProdBaul'].fillna(0, inplace=True)\n",
    "\n",
    "    # Adding the Production values from both dataframes\n",
    "    merged_df['Production'] = merged_df['Production_ProdFreif'] + merged_df['Production_ProdBaul']\n",
    "\n",
    "    # Dropping the individual production columns from each dataframe\n",
    "    merged_df.drop(columns=['Production_ProdFreif', 'Production_ProdBaul'], inplace=True)\n",
    "    print(f'{merged_df[\"Production\"].sum()/1000000000} TWh')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    merged_df.to_csv(f'Data/Results/PV_power_output_{scenario}_{year}.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-30 18:02:28]: Processing for scenario RCP26\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m ws\u001b[38;5;241m=\u001b[39mxr\u001b[38;5;241m.\u001b[39mopen_dataset(variables[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mws\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mformat(scenario\u001b[38;5;241m=\u001b[39mscenario))\n\u001b[0;32m     29\u001b[0m at\u001b[38;5;241m=\u001b[39mxr\u001b[38;5;241m.\u001b[39mopen_dataset(variables[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mformat(scenario\u001b[38;5;241m=\u001b[39mscenario))\n\u001b[1;32m---> 31\u001b[0m germany_gdfSDR \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_germany_gdf_from_ds\u001b[49m\u001b[43m(\u001b[49m\u001b[43msdr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrsds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m germany_gdfWS \u001b[38;5;241m=\u001b[39m create_germany_gdf_from_ds(ws, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msfcWind\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m germany_gdfAT \u001b[38;5;241m=\u001b[39m create_germany_gdf_from_ds(at, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtasmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 25\u001b[0m, in \u001b[0;36mcreate_germany_gdf_from_ds\u001b[1;34m(ds, varname, lat_min, lat_max, lon_min, lon_max)\u001b[0m\n\u001b[0;32m     23\u001b[0m     simplified_time_name \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(time)\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m     data_slice \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39msel(time\u001b[38;5;241m=\u001b[39mtime)\n\u001b[1;32m---> 25\u001b[0m     gdf[simplified_time_name] \u001b[38;5;241m=\u001b[39m \u001b[43mdata_slice\u001b[49m\u001b[43m[\u001b[49m\u001b[43mvarname\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241m.\u001b[39mravel()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Filter the GeoDataFrame for points within the specified geographic bounds\u001b[39;00m\n\u001b[0;32m     28\u001b[0m germany_gdf \u001b[38;5;241m=\u001b[39m gdf[(gdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m lat_min) \u001b[38;5;241m&\u001b[39m (gdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlat\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m lat_max) \u001b[38;5;241m&\u001b[39m (gdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m lon_min) \u001b[38;5;241m&\u001b[39m (gdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlon\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m lon_max)]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xarray\\core\\dataarray.py:784\u001b[0m, in \u001b[0;36mDataArray.values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m    773\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;124;03m    The array's data converted to numpy.ndarray.\u001b[39;00m\n\u001b[0;32m    775\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;124;03m    to this array may be reflected in the DataArray as well.\u001b[39;00m\n\u001b[0;32m    783\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xarray\\core\\variable.py:525\u001b[0m, in \u001b[0;36mVariable.values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalues\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    524\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The variable's data as a numpy.ndarray\"\"\"\u001b[39;00m\n\u001b[1;32m--> 525\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_as_array_or_item\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xarray\\core\\variable.py:323\u001b[0m, in \u001b[0;36m_as_array_or_item\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_as_array_or_item\u001b[39m(data):\n\u001b[0;32m    310\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the given values as a numpy array, or as an individual item if\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;124;03m    it's a 0d datetime64 or timedelta64 array.\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;124;03m    TODO: remove this (replace with np.asarray) once these issues are fixed\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 323\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    324\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    325\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xarray\\core\\indexing.py:806\u001b[0m, in \u001b[0;36mMemoryCachedArray.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: np\u001b[38;5;241m.\u001b[39mtyping\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m--> 806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xarray\\core\\indexing.py:809\u001b[0m, in \u001b[0;36mMemoryCachedArray.get_duck_array\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 809\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39mget_duck_array()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xarray\\core\\indexing.py:803\u001b[0m, in \u001b[0;36mMemoryCachedArray._ensure_cached\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ensure_cached\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray \u001b[38;5;241m=\u001b[39m as_indexable(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xarray\\core\\indexing.py:760\u001b[0m, in \u001b[0;36mCopyOnWriteArray.get_duck_array\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xarray\\core\\indexing.py:630\u001b[0m, in \u001b[0;36mLazilyIndexedArray.get_duck_array\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;66;03m# self.array[self.key] is now a numpy array when\u001b[39;00m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;66;03m# self.array is a BackendArray subclass\u001b[39;00m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;66;03m# and self.key is BasicIndexer((slice(None, None, None),))\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;66;03m# so we need the explicit check for ExplicitlyIndexed\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, ExplicitlyIndexed):\n\u001b[1;32m--> 630\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _wrap_numpy_scalars(array)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xarray\\coding\\variables.py:81\u001b[0m, in \u001b[0;36m_ElementwiseFunctionArray.get_duck_array\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_duck_array\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xarray\\core\\indexing.py:623\u001b[0m, in \u001b[0;36mLazilyIndexedArray.get_duck_array\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    619\u001b[0m     array \u001b[38;5;241m=\u001b[39m apply_indexer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey)\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;66;03m# If the array is not an ExplicitlyIndexedNDArrayMixin,\u001b[39;00m\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;66;03m# it may wrap a BackendArray so use its __getitem__\u001b[39;00m\n\u001b[1;32m--> 623\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;66;03m# self.array[self.key] is now a numpy array when\u001b[39;00m\n\u001b[0;32m    626\u001b[0m \u001b[38;5;66;03m# self.array is a BackendArray subclass\u001b[39;00m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;66;03m# and self.key is BasicIndexer((slice(None, None, None),))\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;66;03m# so we need the explicit check for ExplicitlyIndexed\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, ExplicitlyIndexed):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xarray\\backends\\netCDF4_.py:101\u001b[0m, in \u001b[0;36mNetCDF4ArrayWrapper.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mindexing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplicit_indexing_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIndexingSupport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOUTER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xarray\\core\\indexing.py:987\u001b[0m, in \u001b[0;36mexplicit_indexing_adapter\u001b[1;34m(key, shape, indexing_support, raw_indexing_method)\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Support explicit indexing by delegating to a raw indexing method.\u001b[39;00m\n\u001b[0;32m    966\u001b[0m \n\u001b[0;32m    967\u001b[0m \u001b[38;5;124;03mOuter and/or vectorized indexers are supported by indexing a second time\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;124;03mIndexing result, in the form of a duck numpy-array.\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    986\u001b[0m raw_key, numpy_indices \u001b[38;5;241m=\u001b[39m decompose_indexer(key, shape, indexing_support)\n\u001b[1;32m--> 987\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mraw_indexing_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_key\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtuple\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numpy_indices\u001b[38;5;241m.\u001b[39mtuple:\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;66;03m# index the loaded np.ndarray\u001b[39;00m\n\u001b[0;32m    990\u001b[0m     indexable \u001b[38;5;241m=\u001b[39m NumpyIndexingAdapter(result)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\xarray\\backends\\netCDF4_.py:114\u001b[0m, in \u001b[0;36mNetCDF4ArrayWrapper._getitem\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatastore\u001b[38;5;241m.\u001b[39mlock:\n\u001b[0;32m    113\u001b[0m         original_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_array(needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 114\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mgetitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;66;03m# Catch IndexError in netCDF4 and return a more informative\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# error message.  This is most often called when an unsorted\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# indexer is used before the data is loaded from disk.\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    120\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe indexing operation you are attempting to perform \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    121\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis not valid on netCDF4.Variable object. Try loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data into memory first by calling .load().\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    123\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# plz_shapefile=gpd.read_file('shp/georef-germany-postleitzahl/georef-germany-postleitzahl.shp')\n",
    "# plz_shapefile=plz_shapefile.to_crs('EPSG:4326')\n",
    "# sdr=xr.open_dataset('get_data/PV-Input/rsds_EUR-11_MPI-M-MPI-ESM-LR_rcp26_r1i1p1_SMHI-RCA4_v1a_mon_202101-203012.nc') \n",
    "# ws=xr.open_dataset('get_data/PV-Input/sfcWind_EUR-11_MPI-M-MPI-ESM-LR_rcp26_r1i1p1_SMHI-RCA4_v1a_mon_202101-203012.nc')\n",
    "# at=xr.open_dataset('get_data/PV-Input/tasmax_EUR-11_MPI-M-MPI-ESM-LR_rcp26_r1i1p1_SMHI-RCA4_v1a_mon_202101-203012.nc')\n",
    "# year_oi=2024\n",
    "\n",
    "\n",
    "\n",
    "import xarray as xr\n",
    "import datetime\n",
    "start_year = 2023\n",
    "end_year = 2023\n",
    "# Define the base path and scenarios\n",
    "base_path = \"Data/\"\n",
    "scenarios = ['RCP26', 'RCP45', 'RCP85']\n",
    "variables = {\n",
    "    'sdr': f'rsds_EUR-11_MPI-M-MPI-ESM-LR_{scenario}_r1i1p1_SMHI-RCA4_v1a_mon_202101-205012.nc',\n",
    "    'ws': f'sfcWind_EUR-11_MPI-M-MPI-ESM-LR_{scenario}_r1i1p1_SMHI-RCA4_v1a_mon_202101-205012.nc',\n",
    "    'at': f'tasmax_EUR-11_MPI-M-MPI-ESM-LR_{scenario}_r1i1p1_SMHI-RCA4_v1a_mon_202101-205012.nc'\n",
    "}\n",
    "\n",
    "\n",
    "for scenario in scenarios:\n",
    "    print(f\"[{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}]: Processing for scenario {scenario}\")\n",
    "    #for year in range(start_year, end_year + 1):\n",
    "    sdr=xr.open_dataset(variables['sdr'].format(scenario=scenario))\n",
    "    ws=xr.open_dataset(variables['ws'].format(scenario=scenario))\n",
    "    at=xr.open_dataset(variables['at'].format(scenario=scenario))\n",
    "\n",
    "    germany_gdfSDR = create_germany_gdf_from_ds(sdr, 'rsds')\n",
    "    germany_gdfWS = create_germany_gdf_from_ds(ws, 'sfcWind')\n",
    "    germany_gdfAT = create_germany_gdf_from_ds(at, 'tasmax')\n",
    "\n",
    "    germany_gdfSDR=process_year(germany_gdfSDR, plz_shapefile, 'SDR', scenario)\n",
    "    mapping_dictSDR=generate_id_to_plz_mapping(germany_gdfSDR)   \n",
    "    \n",
    "    germany_gdfWS=process_year(germany_gdfWS,plz_shapefile, 'WS', scenario)\n",
    "    mapping_dictWS=generate_id_to_plz_mapping(germany_gdfWS)   \n",
    "\n",
    "    germany_gdfAT=process_year (germany_gdfAT,plz_shapefile, 'AT', scenario)\n",
    "    mapping_dictAT=generate_id_to_plz_mapping(germany_gdfAT)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contraction data for year 2023 saved, focusing on installations exactly 25 years old.\n",
      "Contraction data for year 2024 saved, focusing on installations exactly 25 years old.\n",
      "Contraction data for year 2025 saved, focusing on installations exactly 25 years old.\n",
      "Contraction data for year 2026 saved, focusing on installations exactly 25 years old.\n",
      "Contraction data for year 2027 saved, focusing on installations exactly 25 years old.\n",
      "Contraction data for year 2028 saved, focusing on installations exactly 25 years old.\n",
      "Contraction data for year 2029 saved, focusing on installations exactly 25 years old.\n",
      "Contraction data for year 2030 saved, focusing on installations exactly 25 years old.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klarw\\AppData\\Local\\Temp\\ipykernel_6512\\1747662707.py:4: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  mastr_data = pd.read_csv(file_path, sep=';')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contraction data for year 2023 saved, focusing on installations exactly 25 years old.\n",
      "Contraction data for year 2024 saved, focusing on installations exactly 25 years old.\n",
      "Contraction data for year 2025 saved, focusing on installations exactly 25 years old.\n",
      "Contraction data for year 2026 saved, focusing on installations exactly 25 years old.\n",
      "Contraction data for year 2027 saved, focusing on installations exactly 25 years old.\n",
      "Contraction data for year 2028 saved, focusing on installations exactly 25 years old.\n",
      "Contraction data for year 2029 saved, focusing on installations exactly 25 years old.\n",
      "Contraction data for year 2030 saved, focusing on installations exactly 25 years old.\n",
      "Number of deprecated installations in 2024: 77\n",
      "Power of deprecated installations in 2024: 1075.0619999999997 kWp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klarw\\AppData\\Local\\Temp\\ipykernel_6512\\1366450366.py:18: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  mastr_pv = pd.read_csv(f'{year_befoistr}MASTR{Typ}.csv', sep=';')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m calculate_annual_contraction(start_year, end_year,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBaul\u001b[39m\u001b[38;5;124m'\u001b[39m, input_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData/pv_interim\u001b[39m\u001b[38;5;124m'\u001b[39m, output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData/pv_interim/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m mastr_pv, expansion_df\u001b[38;5;241m=\u001b[39mfilter_active_installations(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFreif\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;241m2024\u001b[39m, expansion_df) \u001b[38;5;66;03m#passt\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m mastr_pvB,expansion_df\u001b[38;5;241m=\u001b[39m\u001b[43mfilter_active_installations\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mBaul\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mexpansion_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m PV_agg\u001b[38;5;241m=\u001b[39maggregate_on_PLZ_level(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFreif\u001b[39m\u001b[38;5;124m'\u001b[39m, start_year,plz_df)\n\u001b[0;32m     10\u001b[0m PV_aggB\u001b[38;5;241m=\u001b[39maggregate_on_PLZ_level(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBaul\u001b[39m\u001b[38;5;124m'\u001b[39m, start_year,plz_df)\n",
      "Cell \u001b[1;32mIn[40], line 21\u001b[0m, in \u001b[0;36mfilter_active_installations\u001b[1;34m(Typ, year_oi, expansion_df)\u001b[0m\n\u001b[0;32m     18\u001b[0m mastr_pv \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear_befoistr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mMASTR\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTyp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Pre-processing\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m mastr_pv[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInbetriebnahmedatum\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmastr_pv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mInbetriebnahmedatum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mY-\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43mm-\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m mastr_pv[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAge_as_of_year_oi\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m year_oi \u001b[38;5;241m-\u001b[39m mastr_pv[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInbetriebnahmedatum\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39myear\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Filter active and deprecated installations\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\tools\\datetimes.py:1067\u001b[0m, in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m   1065\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n\u001b[0;32m   1066\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1067\u001b[0m         values \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1068\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39m_constructor(values, index\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mindex, name\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1069\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (ABCDataFrame, abc\u001b[38;5;241m.\u001b[39mMutableMapping)):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\tools\\datetimes.py:433\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[1;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_strptime_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m objects_to_datetime64(\n\u001b[0;32m    436\u001b[0m     arg,\n\u001b[0;32m    437\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    441\u001b[0m     allow_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    442\u001b[0m )\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\tools\\datetimes.py:479\u001b[0m, in \u001b[0;36m_array_strptime_with_fallback\u001b[1;34m(arg, name, utc, fmt, exact, errors)\u001b[0m\n\u001b[0;32m    477\u001b[0m     res \u001b[38;5;241m=\u001b[39m Index(result, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM8[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, UTC]\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m--> 479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\indexes\\base.py:475\u001b[0m, in \u001b[0;36mIndex.__new__\u001b[1;34m(cls, data, dtype, copy, name, tupleize_cols)\u001b[0m\n\u001b[0;32m    470\u001b[0m _references \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;66;03m# Constructors\u001b[39;00m\n\u001b[1;32m--> 475\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__new__\u001b[39m(\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m    477\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    478\u001b[0m     dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    479\u001b[0m     copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    480\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    481\u001b[0m     tupleize_cols: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    482\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrange\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RangeIndex\n\u001b[0;32m    485\u001b[0m     name \u001b[38;5;241m=\u001b[39m maybe_extract_name(name, data, \u001b[38;5;28mcls\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_year = 2023\n",
    "end_year = 2030\n",
    "calculate_annual_contraction(start_year, end_year,'Freif', input_dir='Data/pv_interim', output_dir='Data/pv_interim/')\n",
    "calculate_annual_contraction(start_year, end_year,'Baul', input_dir='Data/pv_interim', output_dir='Data/pv_interim/')\n",
    "\n",
    "mastr_pv, expansion_df=filter_active_installations('Freif',2024, expansion_df) #passt\n",
    "mastr_pvB,expansion_df=filter_active_installations('Baul',2024,expansion_df)\n",
    "\n",
    "PV_agg=aggregate_on_PLZ_level('Freif', start_year,plz_df)\n",
    "PV_aggB=aggregate_on_PLZ_level('Baul', start_year,plz_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing year: 2023\n",
      "Index(['plz_code', 'BL', 'Total_BRUTTOLEISTUNG', 'num_installation',\n",
      "       'mean_age', 'BL_code', 'Bundesland', 'Total_Nettonennleistung',\n",
      "       'Anzahl_Anlagen', 'Bundesland_contraction'],\n",
      "      dtype='object')\n",
      "Year: 2023, Installation Type: Freif, Total Chunks Available: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klarw\\AppData\\Local\\Temp\\ipykernel_6512\\1789911382.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['Total_Nettonennleistung'].fillna(0, inplace=True)\n",
      "C:\\Users\\klarw\\AppData\\Local\\Temp\\ipykernel_6512\\1789911382.py:29: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['Anzahl_Anlagen'].fillna(0, inplace=True)\n",
      "C:\\Users\\klarw\\AppData\\Local\\Temp\\ipykernel_6512\\1789911382.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  PV_agg.drop_duplicates(subset='plz_code', keep='first', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal solution found.\n",
      "[2024-05-30 18:58:04.716668]: PV_agg and mastr_pv distributed for year 2023. Sum of power Freifläche: 14980321.456 kWp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klarw\\AppData\\Local\\Temp\\ipykernel_6512\\1789911382.py:3: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  mastr_pv=pd.read_csv(f'2023MASTR{installation_type}.csv', sep=';')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['plz_code', 'BL', 'Total_BRUTTOLEISTUNG', 'num_installation',\n",
      "       'mean_age', 'BL_code', 'Bundesland', 'Total_Nettonennleistung',\n",
      "       'Anzahl_Anlagen', 'Bundesland_contraction'],\n",
      "      dtype='object')\n",
      "Year: 2023, Installation Type: Baul, Total Chunks Available: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klarw\\AppData\\Local\\Temp\\ipykernel_6512\\1789911382.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['Total_Nettonennleistung'].fillna(0, inplace=True)\n",
      "C:\\Users\\klarw\\AppData\\Local\\Temp\\ipykernel_6512\\1789911382.py:29: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['Anzahl_Anlagen'].fillna(0, inplace=True)\n",
      "C:\\Users\\klarw\\AppData\\Local\\Temp\\ipykernel_6512\\1789911382.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  PV_agg.drop_duplicates(subset='plz_code', keep='first', inplace=True)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6512\\840329757.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mprob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPV_agg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdistribute_pv\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0myear\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mexpansion_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Freif'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m]: PV_agg and mastr_pv distributed for year \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0myear_oi\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m. Sum of power Freifläche: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mPV_agg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Total_BRUTTOLEISTUNG'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m kWp\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mprob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPV_aggB\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdistribute_pv\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0myear\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mexpansion_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Baul'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33mf\"\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m]: PV_agg and mastr_pv distributed for year \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0myear_oi\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m. Sum of power Baulich: \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mPV_aggB\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Total_BRUTTOLEISTUNG'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m kWp\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mPV_agg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjoin_mastrs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPV_agg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPV_aggB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmastr_pv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmastr_pvB\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0myear_oi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#passt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6512\\1789911382.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(year, expansion_df, installation_type)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;31m# Define variables for the number of chunks each plz_code can receive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mdistribution_units\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mplz_code\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mPV_agg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'plz_code'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbonsys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbonsys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'plz_code'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mplz_code\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'diff'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbonsys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbonsys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'plz_code'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mplz_code\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minstallation_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Baul'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mbase_upBound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m250\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0minstallation_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Freif'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4089\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4091\u001b[0m         \u001b[1;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4092\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4093\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4094\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4095\u001b[0m         \u001b[1;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4096\u001b[0m         \u001b[1;31m# We interpret tuples as collections only for non-MultiIndex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4151\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4152\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4154\u001b[1;33m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4155\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "# Inner loop for each year in the specified range\n",
    "# mastr_pv, expansion_df=filter_active_installations('Freif',2024, expansion_df) #passt\n",
    "# mastr_pvB,expansion_df=filter_active_installations('Baul',2024,expansion_df) #passt\n",
    "# print('filtering fine')\n",
    "# PV_agg=aggregate_on_PLZ_level('Freif', 2024,plz_df) #passt\n",
    "# PV_aggB=aggregate_on_PLZ_level('Baul', 2024,plz_df) #passt\n",
    "start_year = 2023\n",
    "end_year = 2023\n",
    "for year in range(start_year, end_year + 1):\n",
    "    print(f\"Processing year: {year}\")\n",
    "    year_oi=year\n",
    "\n",
    "    prob, PV_agg=distribute_pv( year,expansion_df,'Freif')\n",
    "    print(f\"[{datetime.datetime.now()}]: PV_agg and mastr_pv distributed for year {year_oi}. Sum of power Freifläche: {PV_agg['Total_BRUTTOLEISTUNG'].sum()} kWp\") \n",
    "\n",
    "    prob, PV_aggB=distribute_pv( year,expansion_df,'Baul')\n",
    "    print(f\"[{datetime.datetime.now()}]: PV_agg and mastr_pv distributed for year {year_oi}. Sum of power Baulich: {PV_aggB['Total_BRUTTOLEISTUNG'].sum()} kWp\") \n",
    "\n",
    "    PV_agg=join_mastrs(PV_agg, PV_aggB, mastr_pv, mastr_pvB,year_oi)#passt\n",
    "    print(f\"[{datetime.datetime.now()}]: PV_agg and mastr_pv joined for year {year_oi} \")\n",
    "\n",
    "\n",
    "    germany_gdfSDR = create_germany_gdf_from_ds(sdr, 'rsds')\n",
    "    germany_gdfWS = create_germany_gdf_from_ds(ws, 'sfcWind')\n",
    "    germany_gdfAT = create_germany_gdf_from_ds(at, 'tasmax')\n",
    "    print(f\"[{datetime.datetime.now()}]: Germany GeoDataFrames created for year {year_oi}\")\n",
    "    \n",
    "    \n",
    "    # germany_gdfSDR=save_years(germany_gdfSDR, 'rsds')\n",
    "    # germany_gdfWS=save_years(germany_gdfWS, 'sfcWind')\n",
    "    # germany_gdfAT=save_years(germany_gdfAT, 'tas')\n",
    "    \n",
    "    \n",
    "    germany_gdfSDR=map_data_to_plz(germany_gdfWS, mapping_dictSDR, 'SDR', year_oi)\n",
    "    germany_gdfWS=map_data_to_plz(germany_gdfWS, mapping_dictWS,'WS', year_oi)\n",
    "    germany_gdfAT=map_data_to_plz(germany_gdfAT, mapping_dictAT,'AT', year_oi)\n",
    "\n",
    "\n",
    "    \n",
    "    print(f'Final{year} done at [{datetime.datetime.now()}]')\n",
    "\n",
    "\n",
    "# Define the base path and scenarios\n",
    "# base_path = \"get_data/PV-Input/\"\n",
    "# scenarios = ['rcp26', 'rcp45', 'rcp85']\n",
    "# variables = {\n",
    "#     'sdr': 'rsds_EUR-11_MPI-M-MPI-ESM-LR_{scenario}_r1i1p1_SMHI-RCA4_v1a_mon_202101-205012.nc',\n",
    "#     'ws': 'sfcWind_EUR-11_MPI-M-MPI-ESM-LR_{scenario}_r1i1p1_SMHI-RCA4_v1a_mon_202101-20501212.nc',\n",
    "#     'at': 'tasmax_EUR-11_MPI-M-MPI-ESM-LR_{scenario}_r1i1p1_SMHI-RCA4_v1a_mon_202101-20501212.nc'\n",
    "# }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for scenario in scenarios:\n",
    "#     germany_gdfAT=pd.read_csv(f'Data/pv_interim/Germany_gdf_AT_{scenario}.csv')\n",
    "#     germany_gdfAT=save_yearly_csv(germany_gdfAT, 2023, 2050, scenario,'germany_gdfAT')\n",
    "#     germany_gdfWS=pd.read_csv(f'Data/pv_interim/Germany_gdf_WS_{scenario}.csv')\n",
    "#     germany_gdfWS=save_yearly_csv(germany_gdfWS, 2023, 2050, scenario,'germany_gdfWS')\n",
    "#     germany_gdfSDR=pd.read_csv(f'Data/pv_interim/Germany_gdf_SDR_{scenario}.csv')\n",
    "#     germany_gdfSDR=save_yearly_csv(germany_gdfSDR, 2023, 2050, scenario,'germany_gdfSDR')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_plz_code_matches(df1, df2, key='plz_code'):\n",
    "    \"\"\"\n",
    "    Check for matching and mismatching PLZ codes between two dataframes.\n",
    "\n",
    "    Parameters:\n",
    "    - df1 (DataFrame): First dataframe to compare.\n",
    "    - df2 (DataFrame): Second dataframe to compare.\n",
    "    - key (str): The column name to compare, default is 'plz_code'.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A dictionary containing sets of matching and mismatching PLZ codes.\n",
    "    \"\"\"\n",
    "    # Convert PLZ codes to integer to ensure comparison accuracy\n",
    "    df1[key] = df1[key].astype(int)\n",
    "    df2[key] = df2[key].astype(int)\n",
    "\n",
    "    # Create sets of PLZ codes from both dataframes\n",
    "    plz_set_df1 = set(df1[key])\n",
    "    plz_set_df2 = set(df2[key])\n",
    "\n",
    "    # Find matches and mismatches\n",
    "    matches = plz_set_df1.intersection(plz_set_df2)\n",
    "    only_in_df1 = plz_set_df1 - plz_set_df2\n",
    "    only_in_df2 = plz_set_df2 - plz_set_df1\n",
    "\n",
    "    # Output the results\n",
    "    print(f\"Matching PLZ codes between the two dataframes: {len(matches)}\")\n",
    "    print(f\"PLZ codes only in the first dataframe: {len(only_in_df1)}\")\n",
    "    print(f\"PLZ codes only in the second dataframe: {len(only_in_df2)}\")\n",
    "\n",
    "    return {\n",
    "        'matches': matches,\n",
    "        'only_in_df1': only_in_df1,\n",
    "        'only_in_df2': only_in_df2\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "result = check_plz_code_matches(bonsys, PV_agg)\n",
    "print(\"Matches:\", result['matches'])\n",
    "print(\"Only in bonsys:\", result['only_in_df1'])\n",
    "print(\"Only in PV_agg:\", result['only_in_df2'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-30 19:26:45]: Processing for scenario RCP26\n",
      "Production in 2023 is 17.486456505305775 TWh for Freif installations\n",
      "Production in 2023 is 44.13703123448 TWh for Baul installations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\klarw\\AppData\\Local\\Temp\\ipykernel_6512\\3722009210.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['Production_ProdFreif'].fillna(0, inplace=True)\n",
      "C:\\Users\\klarw\\AppData\\Local\\Temp\\ipykernel_6512\\3722009210.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  merged_df['Production_ProdBaul'].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.623487739785766 TWh\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/pv_interim/RCP26/germany_gdf_SDR_RCP26_2024.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_year, end_year \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     32\u001b[0m     year_oistr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(year)\n\u001b[1;32m---> 33\u001b[0m     \u001b[43mavg_daily_sunhours\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscenario\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     calculate_Tpv(year,scenario) \n\u001b[0;32m     35\u001b[0m     convert_irradiance_to_PSH( year, scenario)\n",
      "Cell \u001b[1;32mIn[96], line 3\u001b[0m, in \u001b[0;36mavg_daily_sunhours\u001b[1;34m(year_oi, scenario)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mavg_daily_sunhours\u001b[39m(year_oi,scenario):\n\u001b[0;32m      2\u001b[0m     year_oistr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(year_oi)\n\u001b[1;32m----> 3\u001b[0m     germany_gdfSDR \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mData/pv_interim/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mscenario\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/germany_gdf_SDR_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mscenario\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43myear_oi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     timezone \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEurope/Berlin\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m#create empty dataframe to store the results with the columns plz_code, and 01-2024, 02-2024, 03-2024, 04-2024, 05-2024, 06-2024, 07-2024, 08-2024, 09-2024, 10-2024, 11-2024, 12-2024\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/pv_interim/RCP26/germany_gdf_SDR_RCP26_2024.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import datetime\n",
    "\n",
    "# Define the base path and scenarios\n",
    "base_path = \"\"\n",
    "start_year = 2023\n",
    "end_year = 2031\n",
    "variables = {\n",
    "    'sdr': 'rsds_EUR-11_MPI-M-MPI-ESM-LR_{scenario}_r1i1p1_SMHI-RCA4_v1a_mon_{start_year}01-{end_year}12.nc',\n",
    "    'ws': 'sfcWind_EUR-11_MPI-M-MPI-ESM-LR_{scenario}_r1i1p1_SMHI-RCA4_v1a_mon_{start_year}01-{end_year}12.nc',\n",
    "    'at': 'tasmax_EUR-11_MPI-M-MPI-ESM-LR_{scenario}_r1i1p1_SMHI-RCA4_v1a_mon_{start_year}01-{end_year}12.nc'\n",
    "}\n",
    "\n",
    "\n",
    "scenarios = ['RCP26','RCP45', 'RCP85']\n",
    "# # Outermost loop for each RCP scenario\n",
    "# for scenario in scenarios:\n",
    "#     print(f\"[{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}]: Processing for scenario {scenario}\")\n",
    "\n",
    "#     # Inner loop for each year in the specified range\n",
    "#     for year in range(start_year, end_year + 1):\n",
    "#         year_oistr=str(year) \n",
    "#         convert_irradiance_to_PSH( year, scenario)\n",
    "\n",
    "# Outermost loop for each RCP scenario\n",
    "for scenario in scenarios:\n",
    "    print(f\"[{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}]: Processing for scenario {scenario}\")\n",
    "\n",
    "    # Inner loop for each year in the specified range\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        year_oistr=str(year)\n",
    "        avg_daily_sunhours(year, scenario)\n",
    "        calculate_Tpv(year,scenario) \n",
    "        convert_irradiance_to_PSH( year, scenario)\n",
    "        Pout=calculate_actual_power_output(year_oistr,scenario, 'Freif')\n",
    "        PoutB=calculate_actual_power_output(year_oistr,scenario, 'Baul')\n",
    "        merge_Power_outputs(year,scenario)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
