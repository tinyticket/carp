{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "####################################################################################################\n",
    "##########                      C\tlimate-Change                                       ############\n",
    "##########                      A\tssessment Model for                                 ############\n",
    "##########                      R\tenewable Energy                                     ############\n",
    "##########                      P\tower output                                         ############\n",
    "##########                                                                              ############\n",
    "####################################################################################################\n",
    "####################################################################################################\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point\n",
    "import numpy as np\n",
    "import os\n",
    "import pulp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import weibull_min\n",
    "import tqdm\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create directory structure\n",
    "def setup_directory_structure(base_path):\n",
    "    scenarios = ['RCP2.6', 'RCP4.5', 'RCP8.5']\n",
    "    subfolders = ['pv_interim', 'wind_interim', 'Results']\n",
    "\n",
    "    # Iterate over each subfolder and scenario to create the directory tree\n",
    "    for subfolder in subfolders:\n",
    "        for scenario in scenarios:\n",
    "            # Construct the full path for the directory\n",
    "            dir_path = Path(base_path) / 'Data' / subfolder / scenario\n",
    "            # Create the directory if it doesn't exist\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "\n",
    "base_path = Path(os.getcwd())\n",
    "\n",
    "\n",
    "# Setup the directory structure based on the base location\n",
    "setup_directory_structure(base_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_curves = pd.read_csv(f'powercurves.csv', sep=\t';')\n",
    "germany_gdf_template = None\n",
    "plz_shapefile=gpd.read_file('georef-germany-postleitzahl.shp')\n",
    "\n",
    "# Create a DataFrame from 2023 to 2050 with each year increasing by 10000 in expansion\n",
    "pathways = {\n",
    "    \"Year\": [2021,2022,2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040,2041,2042,2043,2044,2045,2046,2047,2048,2049,2050],\n",
    "    \"Wind On-Shore Expansion\": [0.0,0.0,0.0, 9.2, 7.5, 7.5, 7.5, 7.5, 8.0, 8.0, 8.4, 8.4, 8.4, 8.4, 8.4, 0.6, 0.6, 0.6, 0.6, 0.6,0,0,0,0,0,0,0,0,0,0]\n",
    "}\n",
    "expansion_df = pd.DataFrame({\n",
    "    \"Year\": pathways[\"Year\"],\n",
    "    \"Wind On-Shore Expansion\": pathways[\"Wind On-Shore Expansion\"]\n",
    "})\n",
    "expansion_df['Wind On-Shore Expansion'] = expansion_df['Wind On-Shore Expansion'] * 1000\n",
    "expansion_df.to_csv(f'Data/expansion2023.csv', sep=';', index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_shapefiles_with_wind_data_optimized(plz_shapefile, wind_gdf):\n",
    "        \n",
    "        # Read the PLZ shapefile\n",
    "        plz_gdf = plz_shapefile.copy()\n",
    "        #adjust the crs to a metric crs as this will help the buffer\n",
    "        wind_gdf.crs = 'epsg:4647'\n",
    "        plz_gdf.crs='epsg:4647'\n",
    "        \n",
    "\n",
    "        # Define time columns and initialize new columns in plz_gdf\n",
    "        time_cols = [col for col in wind_gdf.columns if col not in ['geometry']]\n",
    "        for time_col in time_cols:\n",
    "            plz_gdf[f'wind_speed_mean_{time_col}'] = None\n",
    "        plz_gdf['point_ids'] = None\n",
    "        plz_gdf['point_count'] = 0\n",
    "        plz_gdf['type'] = None\n",
    "        plz_gdf['closest_point_distance'] = None\n",
    "\n",
    "        # Spatial join for points within polygons\n",
    "        joined_gdf = gpd.sjoin(plz_gdf, wind_gdf, how='left', op='contains')\n",
    "        for idx, group in joined_gdf.groupby(joined_gdf.index):\n",
    "            wind_points = group.dropna(subset=['ID'])\n",
    "            plz_gdf.at[idx, 'point_ids'] = list(wind_points['ID'].astype(int))\n",
    "            plz_gdf.at[idx, 'point_count'] = len(wind_points)\n",
    "            plz_gdf.at[idx, 'type'] = 'within' if len(wind_points) > 0 else None\n",
    "            for time_col in time_cols:\n",
    "                plz_gdf.at[idx, f'wind_speed_mean_{time_col}'] = wind_points[time_col].mean()\n",
    "\n",
    "        # Optimized handling of PLZ polygons without wind points\n",
    "        no_wind_plz = plz_gdf[plz_gdf['point_count'] == 0]\n",
    "        buffer_distance = 14000  # 14 km buffer\n",
    "        wind_sindex = wind_gdf.sindex  # Spatial index for wind_gdf\n",
    "\n",
    "        for idx, row in no_wind_plz.iterrows():\n",
    "            # Use spatial index to narrow down the candidates\n",
    "            potential_matches_index = list(wind_sindex.intersection(row.geometry.buffer(buffer_distance).bounds))\n",
    "            potential_matches = wind_gdf.iloc[potential_matches_index]\n",
    "            # Calculate the nearest point\n",
    "            nearest_point_data = potential_matches.distance(row.geometry).idxmin()\n",
    "            nearest_point = wind_gdf.loc[nearest_point_data] if pd.notna(nearest_point_data) else None\n",
    "\n",
    "            if nearest_point is not None:\n",
    "                nearest_point_id = nearest_point.name\n",
    "                plz_gdf.at[idx, 'point_ids'] = [nearest_point_id]\n",
    "                plz_gdf.at[idx, 'point_count'] = 1\n",
    "                plz_gdf.at[idx, 'type'] = 'nearby'\n",
    "                plz_gdf.at[idx, 'closest_point_distance'] = row.geometry.distance(nearest_point.geometry)\n",
    "                for time_col in time_cols:\n",
    "                    plz_gdf.at[idx, f'wind_speed_mean_{time_col}'] = nearest_point[time_col]\n",
    "        return plz_gdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_germany_gdf_from_ds(ds, lat_min=46, lat_max=56, lon_min=5, lon_max=16):\n",
    "    \"\"\"\n",
    "    Creates a GeoDataFrame containing points within Germany's geographic bounds from a given dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - ds: The dataset containing latitude, longitude, and wind speed data.\n",
    "    - lat_min, lat_max, lon_min, lon_max: Geographic bounds for filtering the points.\n",
    "    \n",
    "    Returns:\n",
    "    - A GeoDataFrame with points within the specified bounds and wind speed data for each time step.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame from dataset coordinates\n",
    "    lat_lon_df = pd.DataFrame({\n",
    "        'lat': ds['lat'].values.ravel(),\n",
    "        'lon': ds['lon'].values.ravel()\n",
    "    })\n",
    "\n",
    "    # Create a GeoDataFrame from lat_lon_df\n",
    "    gdf = gpd.GeoDataFrame(lat_lon_df, geometry=gpd.points_from_xy(lat_lon_df.lon, lat_lon_df.lat))\n",
    "\n",
    "    # Add each time step as a new column with simplified name 'year-month'\n",
    "    for time in ds['time'].values:\n",
    "        simplified_time_name = pd.to_datetime(time).strftime('%Y-%m')\n",
    "        data_slice = ds.sel(time=time)\n",
    "        gdf[simplified_time_name] = data_slice['sfcWind'].values.ravel()\n",
    "\n",
    "    # Filter the GeoDataFrame for points within the specified geographic bounds\n",
    "    germany_gdf = gdf[(gdf['lat'] >= lat_min) & (gdf['lat'] <= lat_max) & (gdf['lon'] >= lon_min) & (gdf['lon'] <= lon_max)]\n",
    "    germany_gdf['ID'] = germany_gdf.index\n",
    "    \n",
    "    return germany_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_shapefiles_with_wind_data_optimized(plz_shapefile, wind_gdf):\n",
    "        \n",
    "        # Read the PLZ shapefile\n",
    "        plz_gdf = plz_shapefile.copy()\n",
    "        #adjust the crs to a metric crs as this will help the buffer\n",
    "        wind_gdf.crs = 'epsg:4647'\n",
    "        plz_gdf.crs='epsg:4647'\n",
    "        \n",
    "\n",
    "        # Define time columns and initialize new columns in plz_gdf\n",
    "        time_cols = [col for col in wind_gdf.columns if col not in ['geometry']]\n",
    "        for time_col in time_cols:\n",
    "            plz_gdf[f'wind_speed_mean_{time_col}'] = None\n",
    "        plz_gdf['point_ids'] = None\n",
    "        plz_gdf['point_count'] = 0\n",
    "        plz_gdf['type'] = None\n",
    "        plz_gdf['closest_point_distance'] = None\n",
    "\n",
    "        # Spatial join for points within polygons\n",
    "        joined_gdf = gpd.sjoin(plz_gdf, wind_gdf, how='left', op='contains')\n",
    "        for idx, group in joined_gdf.groupby(joined_gdf.index):\n",
    "            wind_points = group.dropna(subset=['ID'])\n",
    "            plz_gdf.at[idx, 'point_ids'] = list(wind_points['ID'].astype(int))\n",
    "            plz_gdf.at[idx, 'point_count'] = len(wind_points)\n",
    "            plz_gdf.at[idx, 'type'] = 'within' if len(wind_points) > 0 else None\n",
    "            for time_col in time_cols:\n",
    "                plz_gdf.at[idx, f'wind_speed_mean_{time_col}'] = wind_points[time_col].mean()\n",
    "\n",
    "        # Optimized handling of PLZ polygons without wind points\n",
    "        no_wind_plz = plz_gdf[plz_gdf['point_count'] == 0]\n",
    "        buffer_distance = 14000  # 14 km buffer\n",
    "        wind_sindex = wind_gdf.sindex  # Spatial index for wind_gdf\n",
    "\n",
    "        for idx, row in no_wind_plz.iterrows():\n",
    "            # Use spatial index to narrow down the candidates\n",
    "            potential_matches_index = list(wind_sindex.intersection(row.geometry.buffer(buffer_distance).bounds))\n",
    "            potential_matches = wind_gdf.iloc[potential_matches_index]\n",
    "            # Calculate the nearest point\n",
    "            nearest_point_data = potential_matches.distance(row.geometry).idxmin()\n",
    "            nearest_point = wind_gdf.loc[nearest_point_data] if pd.notna(nearest_point_data) else None\n",
    "\n",
    "            if nearest_point is not None:\n",
    "                nearest_point_id = nearest_point.name\n",
    "                plz_gdf.at[idx, 'point_ids'] = [nearest_point_id]\n",
    "                plz_gdf.at[idx, 'point_count'] = 1\n",
    "                plz_gdf.at[idx, 'type'] = 'nearby'\n",
    "                plz_gdf.at[idx, 'closest_point_distance'] = row.geometry.distance(nearest_point.geometry)\n",
    "                for time_col in time_cols:\n",
    "                    plz_gdf.at[idx, f'wind_speed_mean_{time_col}'] = nearest_point[time_col]\n",
    "        return plz_gdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#germany_gdf=create_germany_gdf_from_ds(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_year_for_germany(germany_gdf_template, paths):\n",
    "    results = []\n",
    "    \n",
    "    for path in paths:\n",
    "        # Load the dataset\n",
    "        ds = xr.open_dataset(path)\n",
    "        \n",
    "        # Create a base DataFrame from the coordinates (if not already created)\n",
    "        if germany_gdf_template is None:\n",
    "            lat_lon_df = pd.DataFrame({\n",
    "                'lat': ds['lat'].values.ravel(),\n",
    "                'lon': ds['lon'].values.ravel()\n",
    "            })\n",
    "            germany_gdf = gpd.GeoDataFrame(lat_lon_df, geometry=gpd.points_from_xy(lat_lon_df.lon, lat_lon_df.lat))\n",
    "        else:\n",
    "            germany_gdf = germany_gdf_template.copy()\n",
    "        \n",
    "        # Extract all years data for 'sfcWind'\n",
    "        for time in ds['time'].values:\n",
    "            simplified_time_name = pd.to_datetime(time).strftime('%Y-%m')\n",
    "            data_slice = ds.sel(time=time)\n",
    "            germany_gdf[simplified_time_name] = data_slice['sfcWind'].values.ravel()\n",
    "        \n",
    "        # Add dataset identifier based on the path (to distinguish between RCP scenarios)\n",
    "        rcp_id = path.split('_')[3]  # Assumes RCP info is the 4th element in the filename\n",
    "        germany_gdf['RCP'] = rcp_id\n",
    "        \n",
    "        results.append(germany_gdf)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "called_function"
    ]
   },
   "outputs": [],
   "source": [
    "#TODO: include RCP stuff!!\n",
    "# Assuming you have defined the function as provided in the previous code snippet\n",
    "\n",
    "# Define the paths to your NetCDF files for each RCP scenario\n",
    "paths = [\n",
    "    'get_data/sfcWind_EUR-11_MPI-M-MPI-ESM-LR_rcp26_r1i1p1_SMHI-RCA4_v1a_mon_202101-205012.nc',\n",
    "    'get_data/sfcWind_EUR-11_MPI-M-MPI-ESM-LR_rcp45_r1i1p1_SMHI-RCA4_v1a_mon_202101-205012.nc',\n",
    "    'get_data/sfcWind_EUR-11_MPI-M-MPI-ESM-LR_rcp85_r1i1p1_SMHI-RCA4_v1a_mon_202101-205012.nc'\n",
    "]\n",
    "\n",
    "# Specify the year of interest\n",
    "#year_oi = 2024  # Example year\n",
    "\n",
    "# Initial call to the function (assuming germany_gdf_template is None for the first call)\n",
    "germany_gdf_template = None  # This will be generated inside the function\n",
    "\n",
    "# Call the function with the specified year and paths\n",
    "#processed_data = process_year_for_germany(year_oi, germany_gdf_template, paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_year(germany_gdf,scenario):\n",
    "    # Load PLZ shapefile\n",
    "    plz = gpd.read_file('georef-germany-postleitzahl.shp')\n",
    "\n",
    "    # Assuming the process_shapefiles_with_wind_data_optimized returns the full DataFrame with multiple years data\n",
    "    updated_plz_gdf = process_shapefiles_with_wind_data_optimized(plz, germany_gdf)\n",
    "    print(\"updated_plz_gdf erzeugt\")\n",
    "\n",
    "    #Drop unnecessary columns\n",
    "    #updated_plz_gdf.drop(['name', 'plz_name', 'plz_name_lo', 'krs_code', 'lan_name', 'lan_code', 'krs_name', 'PLZ_int'], axis=1, inplace=True)\n",
    "    \n",
    "    # Rename columns as needed\n",
    "    updated_plz_gdf.rename(columns={'wind_speed_mean_ID':'ID', 'wind_speed_mean_lat':'lat', 'wind_speed_mean_lon':'lon'}, inplace=True)\n",
    "    # Select only the necessary columns\n",
    "    #updated_plz_gdf = updated_plz_gdf[['plz_code', 'BL', 'GrPow_MW', 'Anzahl_Anlagen', 'avg_RD', 'avg_NH']]\n",
    "\n",
    "    # Remove duplicates\n",
    "    updated_plz_gdf.drop_duplicates(subset='plz_code', keep='first', inplace=True)\n",
    "\n",
    "    # Extract years from column names and save separate CSVs for each year\n",
    "    years = set(col.split('-')[0] for col in updated_plz_gdf.columns if '-' in col)\n",
    "    for year in years:\n",
    "        # Select columns for the specific year along with ID, plz_code, lat, lon, and geometry\n",
    "        columns = ['ID', 'plz_code', 'lat', 'lon', 'geometry'] + [col for col in updated_plz_gdf.columns if col.startswith(year)]\n",
    "        year_df = updated_plz_gdf[columns]\n",
    "\n",
    "        # Save the DataFrame for this year into a CSV file\n",
    "        csv_path = f'Data/wind_interimupdated_plz_{scenario}_{year}.csv'\n",
    "        year_df.to_csv(csv_path, index=False)\n",
    "        print(f\"Data for {year} saved to {csv_path}\")\n",
    "\n",
    "    return updated_plz_gdf\n",
    "\n",
    "\n",
    "#updated_plz_gdf = process_year(year_oi, germany_gdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_annual_contraction(start_year, end_year, input_dir='Data/', output_dir='Data/wind_interimcontraction'):\n",
    "    # Load the combined MASTR data\n",
    "    file_path = f'{input_dir}/2023_MASTR_ALR.csv'\n",
    "    mastr_data = pd.read_csv(file_path, sep=';')\n",
    "    mastr_data['INBETRIEBNAHMEDATUM'] = pd.to_datetime(mastr_data['INBETRIEBNAHMEDATUM'])\n",
    "    mastr_data = mastr_data.dropna(subset=['INBETRIEBNAHMEDATUM'])\n",
    "    mastr_data = mastr_data[mastr_data.EINHEITBETRIEBSSTATUS != 'EndgueltigStillgelegt']\n",
    "    # Loop through each year to calculate decommissioning for exactly 20-year-old turbines\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        # Filter turbines that turn exactly 20 years old during the year\n",
    "        yearly_decommissioned = mastr_data[mastr_data['IBN_YEAR']== (year - 20)]\n",
    "        \n",
    "\n",
    "        # Aggregate the decommissioned data by postcode\n",
    "        aggregated_data = yearly_decommissioned.groupby('POSTLEITZAHL').agg(\n",
    "            Total_BRUTTOLEISTUNG=('BRUTTOLEISTUNG', 'sum'),\n",
    "            Anzahl_Anlagen=('POSTLEITZAHL', 'count'),\n",
    "            avg_RD=('RD_NEW', 'mean'),  # Assuming 'RD_NEW' is the column for Rotor Diameter\n",
    "            avg_NH=('NH_new', 'mean'),  # Assuming 'NH_new' is the column for Hub Height\n",
    "            Bundesland=('BUNDESLAND', 'first')\n",
    "        ).reset_index()\n",
    "\n",
    "        #rename POSTLEITZAHL to plz_code\n",
    "        aggregated_data.rename(columns={'POSTLEITZAHL':'plz_code'}, inplace=True)\n",
    "        aggregated_data.rename(columns={'Total_BRUTTOLEISTUNG':'GrPow_MW'}, inplace=True)\n",
    "        # Save the yearly data\n",
    "        aggregated_data.to_csv(f'{output_dir}/contraction_data_{year}.csv', sep=';', index=False)\n",
    "        print(f'Contraction data for year {year} saved')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_height_df( year_oi, input_dir='Data/', output_dir='Data/Wind_interim'):\n",
    "    \"\"\"\n",
    "    Loads MaStR data for the specified year, prepares contraction data, and saves the processed DataFrame.\n",
    "    Parameters:\n",
    "    - year_befoistr: The year before the year of interest, as a string or integer.\n",
    "    - year_oi: The year of interest, used for calculating the age of installations, as a string or integer.\n",
    "    - input_dir: Directory path where the input data files are located.\n",
    "    - output_dir: Directory path where the output data files will be saved.\n",
    "    Returns:\n",
    "    - DataFrame that includes turbine and power data aggregated by postcode.\n",
    "    \"\"\"\n",
    "    # Load expansion data for the specified year\n",
    " \n",
    "    # Load MaStR data\n",
    "    file_path = f'{input_dir}/2023_MASTR_ALR.csv'\n",
    "    contraction = pd.read_csv(file_path, sep=';')\n",
    "\n",
    "    # Select relevant columns and clean data\n",
    "    contraction = contraction[['EINHEITMASTRNUMMER','BUNDESLAND', 'LANDKREIS', 'GEMEINDE', 'POSTLEITZAHL', 'MELDEDATUM', 'INBETRIEBNAHMEDATUM', 'GEPLANTESINBETRIEBNAHMEDATUM', 'EINHEITBETRIEBSSTATUS', 'BRUTTOLEISTUNG', 'NH_new', 'RD_NEW', 'UTM32_EAST', 'UTM32_NORTH']]\n",
    "    contraction['INBETRIEBNAHMEDATUM'] = pd.to_datetime(contraction['INBETRIEBNAHMEDATUM']).fillna(pd.to_datetime(contraction['GEPLANTESINBETRIEBNAHMEDATUM'], errors='coerce',dayfirst=True))\n",
    "    #contraction = contraction.dropna(subset=['INBETRIEBNAHMEDATUM'])\n",
    "    contraction = contraction[contraction.EINHEITBETRIEBSSTATUS != 'EndgueltigStillgelegt']\n",
    "    contraction['Age_as_of_year_oi'] = year_oi - contraction['INBETRIEBNAHMEDATUM'].dt.year\n",
    "    contraction = contraction[contraction.Age_as_of_year_oi <= 21]\n",
    "   \n",
    "\n",
    "\n",
    "    # Aggregate contraction data by PLZ for relevant metrics\n",
    "    height_df = contraction.groupby('POSTLEITZAHL').agg(\n",
    "        BL=('BUNDESLAND', 'first'),\n",
    "        GrPow_MW=('BRUTTOLEISTUNG', 'sum'),\n",
    "        Anzahl_Anlagen=('POSTLEITZAHL', 'count'),\n",
    "        avg_RD=('RD_NEW', 'mean'),\n",
    "        avg_NH=('NH_new', 'mean')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Update column names and convert power from kW to MW\n",
    "    height_df.rename(columns={'POSTLEITZAHL': 'plz_code', 'GrPow_MW': 'GrPow_MW'}, inplace=True)\n",
    "    height_df.to_csv(f'{output_dir}/height_data_2023.csv', sep=';', index=False )\n",
    "\n",
    "    return height_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_expansion_randomly(year,expansion_chunk_value):\n",
    "    year_befoistr=str(year)\n",
    "    # Define the decrease in area per chunk distributed\n",
    "    year_decom=year+20\n",
    "\n",
    "    blpath='Data/BL-Area_'+year_befoistr+'.csv'\n",
    "    BL_Area=pd.read_csv(blpath,sep=';')\n",
    "    height_df=pd.read_csv(f'Data/wind_interim/height_data_{year}.csv',sep=';')\n",
    "\n",
    "    height_df=height_df.replace([np.inf, -np.inf], np.nan)\n",
    "    height_df=height_df.dropna(subset=['avg_RD'])\n",
    "    height_df=height_df.dropna(subset=['avg_NH'])\n",
    "    height_df=height_df.dropna(subset=['GrPow_MW'])\n",
    "\n",
    "    average_avg_RD = height_df['avg_RD'].mean()\n",
    "    average_avg_NH = height_df['avg_NH'].mean()\n",
    "    rd_value=3.48498987*year-6915.90402785 #Regression line for RD from Mastr data, RMSE: 15.8333, R2: 0.7646\n",
    "    nh_value=0.8523*rd_value+26.6221 #Regression line for NH from Mastr data, RMSE: 19.71183, R2: 0.66930\n",
    "    expansion_df=pd.read_csv(f'Data/expansion2023.csv',sep=';')\n",
    "    contraction_df=pd.read_csv(f'Data/wind_interim/contraction_data_{year}.csv',sep=';')\n",
    "    \n",
    "    area_decrease_per_chunk = 3.14159265*((average_avg_RD**2)+900)/1000000\n",
    "\n",
    "\n",
    "    add_to_expansion=contraction_df['GrPow_MW'].sum()\n",
    "    \n",
    "    area_increase = contraction_df.groupby('Bundesland').agg(\n",
    "        Mean_RD_Contraction=('avg_RD', 'mean'),\n",
    "        Total_Anlagen_Contraction=('Anzahl_Anlagen', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Calculate the increase in area for each Bundesland based on the mean RD contraction\n",
    "    area_increase['AreaIncrease'] = 3.14159265 * ((area_increase['Mean_RD_Contraction']**2) + 900) / 1000000\n",
    "\n",
    "    # Multiply the 'AreaIncrease' by the number of contracted installations to get total area to increase\n",
    "    area_increase['TotalAreaIncrease'] = area_increase['AreaIncrease'] * area_increase['Total_Anlagen_Contraction']    \n",
    "    \n",
    "    # Setting index for BL_Area if not already done\n",
    "    if BL_Area.index.name != 'Bundesland':\n",
    "        BL_Area.set_index('Bundesland', inplace=True)\n",
    "\n",
    "    #add the area increase to the BL_Area DataFrame\n",
    "    for index, row in area_increase.iterrows():\n",
    "        BL_Area.at[row['Bundesland'], 'Freie Flaeche'] += row['TotalAreaIncrease']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    total_expansion_value1=expansion_df[expansion_df['Year']==year]['Wind On-Shore Expansion'].values[0]\n",
    "    total_expansion_value=add_to_expansion+total_expansion_value1\n",
    "    #print(f'total_expansion_value: {total_expansion_value}, add_to_expansion: {add_to_expansion}, EEG_expansion_value: {total_expansion_value1}  ')\n",
    "\n",
    "    # Merge the dataframes on 'plz_code'\n",
    "    merged_df = pd.merge(height_df, contraction_df, on='plz_code', how='left', suffixes=('', '_contraction'))\n",
    "\n",
    "    # Fill NaN values with 0 for contraction data (assuming no contraction means 0)\n",
    "    merged_df['GrPow_MW_contraction'].fillna(0, inplace=True)\n",
    "    merged_df['Anzahl_Anlagen_contraction'].fillna(0, inplace=True)\n",
    "    merged_df['avg_RD_contraction'].fillna(0, inplace=True)\n",
    "    merged_df['avg_NH_contraction'].fillna(0, inplace=True)  # Fill with average if no data\n",
    "    \n",
    "    # Calculate total contributions before contraction\n",
    "    merged_df['total_RD_before'] = merged_df['avg_RD'] * merged_df['Anzahl_Anlagen']\n",
    "    merged_df['total_NH_before'] = merged_df['avg_NH'] * merged_df['Anzahl_Anlagen']\n",
    "    # Subtract the contraction values\n",
    "    merged_df['GrPow_MW'] = merged_df['GrPow_MW'] - merged_df['GrPow_MW_contraction']\n",
    "    # print(f'GrPow_MW after subtraction: {merged_df[\"GrPow_MW\"].sum()}')\n",
    "    merged_df['Anzahl_Anlagen'] = merged_df['Anzahl_Anlagen'] - merged_df['Anzahl_Anlagen_contraction']\n",
    "    merged_df['total_RD_after'] = merged_df['total_RD_before'] - (merged_df['avg_RD_contraction'] * merged_df['Anzahl_Anlagen_contraction'])\n",
    "    merged_df['total_NH_after'] = merged_df['total_NH_before'] - (merged_df['avg_NH_contraction'] * merged_df['Anzahl_Anlagen_contraction'])\n",
    "    \n",
    " \n",
    "    # Calculate avg_RD and avg_NH, defaulting to 0 when Anzahl_Anlagen is 0\n",
    "    merged_df['avg_RD'] = np.where(merged_df['Anzahl_Anlagen'] > 0, \n",
    "                                merged_df['total_RD_after'] / merged_df['Anzahl_Anlagen'], \n",
    "                                0)\n",
    "    merged_df['avg_NH'] = np.where(merged_df['Anzahl_Anlagen'] > 0, \n",
    "                                merged_df['total_NH_after'] / merged_df['Anzahl_Anlagen'], \n",
    "                                0)\n",
    "    #print smallest value of avg_rd, avg_NH and GrPow_MW\n",
    "    # print(f'Smallest avg_RD: {merged_df[\"avg_RD\"].min()}')\n",
    "    # print(f'Smallest avg_NH: {merged_df[\"avg_NH\"].min()}')\n",
    "    # print(f'Smallest GrPow_MW: {merged_df[\"GrPow_MW\"].min()}')\n",
    "    \n",
    "    # Drop the contraction columns if they are no longer needed\n",
    "    merged_df.drop(columns=['GrPow_MW_contraction', 'Anzahl_Anlagen_contraction', 'avg_RD_contraction', 'avg_NH_contraction', 'total_RD_before', 'total_NH_before', 'total_RD_after', 'total_NH_after'], inplace=True)\n",
    "\n",
    "    height_df=merged_df #Elias empfiehlt: merged_df.copy()\n",
    "\n",
    "\n",
    "    print(f'GrPow_MW as HDF: {height_df[\"GrPow_MW\"].sum()}')\n",
    "    \n",
    "    # Create a new LP problem\n",
    "    prob = pulp.LpProblem(\"RandomDistributionOfExpansion\", pulp.LpMaximize)\n",
    "\n",
    "    # Total number of chunks available\n",
    "    total_chunks = int(total_expansion_value / expansion_chunk_value)\n",
    "    # print(f'total_chunks: {total_chunks}')\n",
    "    # print(f\"Year of Interest: {year}, Selected Expansion Value: {total_expansion_value}\")\n",
    "    # Variables: Number of chunks to distribute to each plz_code\n",
    "    chunks = pulp.LpVariable.dicts(\"Chunks\",\n",
    "                                   (index for index in height_df.index),\n",
    "                                   lowBound=0,\n",
    "                                   upBound=4,  #max 10 installations per plz_code\n",
    "                                   cat='Integer')\n",
    "\n",
    "    # Constraint: Sum of all distributed chunks should not exceed total available chunks\n",
    "    prob += pulp.lpSum([chunks[i] for i in height_df.index]) <= total_chunks, \"TotalExpansionLimit\"\n",
    "   \n",
    "    # Area constraints for each plz_code\n",
    "    for index, row in height_df.iterrows():\n",
    "        bl = row['BL']  # Get the region identifier\n",
    "        # Maximum area available for this plz_code, fetched from BL_Area DataFrame\n",
    "        available_area = BL_Area.at[bl, 'Freie Flaeche']\n",
    "        # Ensuring that the distributed units do not exceed the available area\n",
    "        prob += chunks[index] * area_decrease_per_chunk <= available_area, f\"AreaConstraint_{index}\"\n",
    "    \n",
    "    # Objective: Distribute as evenly as possible, maximize the number of plz_code receiving chunks\n",
    "    prob += pulp.lpSum([chunks[i] for i in height_df.index])\n",
    "\n",
    "    # Solve the problem\n",
    "    prob.solve()\n",
    "\n",
    "    \n",
    "   # Output results and update logic\n",
    "    distribution_records = []\n",
    "    if pulp.LpStatus[prob.status] == 'Optimal':\n",
    "        # print(\"Distribution Completed Successfully\")\n",
    "        # print(\"Solver Status:\", pulp.LpStatus[prob.status])\n",
    "        \n",
    "        total_distributed_chunks = sum(int(var.value()) for var in chunks.values())\n",
    "        print(f\"Total distributed chunks: {total_distributed_chunks} out of available {total_chunks}\")\n",
    "        \n",
    "        accumulated_distributed_chunks = 0  # Initialize at 0 to accumulate correctly\n",
    "        accumulated_distributed_power = 0  # Initialize at 0 to accumulate the total power added\n",
    "\n",
    "        for index, row in height_df.iterrows():\n",
    "            distributed_chunks = int(pulp.value(chunks[index]))\n",
    "            \n",
    "            accumulated_distributed_chunks += distributed_chunks  # Accumulate distributed chunks\n",
    "            \n",
    "            if distributed_chunks > 0:\n",
    "                additional_power = distributed_chunks * expansion_chunk_value\n",
    "                height_df.at[index, 'Anzahl_Anlagen'] += distributed_chunks\n",
    "                height_df.at[index, 'GrPow_MW'] += additional_power\n",
    "                # Recalculate averages based on new totals\n",
    "                total_rd = height_df.at[index, 'avg_RD'] * (height_df.at[index, 'Anzahl_Anlagen'] - distributed_chunks)\n",
    "                total_nh = height_df.at[index, 'avg_NH'] * (height_df.at[index, 'Anzahl_Anlagen'] - distributed_chunks)\n",
    "                total_rd += rd_value * distributed_chunks\n",
    "                total_nh += nh_value * distributed_chunks\n",
    "                height_df.at[index, 'avg_RD'] = total_rd / height_df.at[index, 'Anzahl_Anlagen']\n",
    "                height_df.at[index, 'avg_NH'] = total_nh / height_df.at[index, 'Anzahl_Anlagen']\n",
    "                accumulated_distributed_power += additional_power  # Add the power for these chunks to the total\n",
    "                distribution_records.append({\n",
    "                    'plz_code': row['plz_code'],\n",
    "                    'Bundesland': row['BL'],\n",
    "                    'GrPow_MW': additional_power,\n",
    "                    'Anzahl_Anlagen': distributed_chunks,\n",
    "                    'avg_RD': rd_value,\n",
    "                    'avg_NH': nh_value\n",
    "                })\n",
    "        newsum = height_df['GrPow_MW'].sum()\n",
    "        # print(f'maximum Gross Power: {height_df['GrPow_MW'].max()}')\n",
    "        # print(f\"Optimization was successful. New Gross Power (MW): {newsum} MW\")\n",
    "        # print(f\"Chunks distributed: {accumulated_distributed_chunks} out of {total_chunks}\")\n",
    "        # print(f\"Total power added: {accumulated_distributed_power} MW\")\n",
    "        height_df.to_csv(f'Data/wind_interimheight_data_{year+1}.csv', sep=';', index=False)\n",
    "        distribution_records = pd.DataFrame(distribution_records)\n",
    "        distribution_records.to_csv(f'Data/wind_interim/contraction_data_{year_decom}.csv', sep=';', index=False)\n",
    "    else:\n",
    "        print(\"Failed to find an optimal solution.\")\n",
    "    return distribution_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_wind_speed_data(scenario,year_oi, year_oistr, output_dir):\n",
    "    \"\"\"\n",
    "    Processes wind speed data, interpolates wind speed at hub height, and saves the data to CSV files.\n",
    "\n",
    "    Parameters:\n",
    "    - updated_plz_gdf: GeoDataFrame with PLZ data updated.\n",
    "    - height_df: DataFrame containing height data for wind turbines.\n",
    "    - year_oistr: Year of interest as a string.\n",
    "    - output_dir: Directory path where the output CSV files will be saved.\n",
    "    \"\"\"\n",
    "    height_df=pd.read_csv(f'Data/wind_interimheight_data_{year_oi}.csv', sep=';')\n",
    "    updated_plz_gdf=pd.read_csv(f'Data/wind_interimupdated_plz_{scenario}_wind_speed_mean_{year_oi}.csv')\n",
    "    #drop rows in height df where Bruttoleistung, avg_NH or avg_RD is nan\n",
    "    height_df=height_df.replace([np.inf, -np.inf], np.nan)\n",
    "    height_df=height_df.dropna(subset=['avg_RD'])\n",
    "    height_df=height_df.dropna(subset=['avg_NH'])\n",
    "    height_df=height_df.dropna(subset=['GrPow_MW'])\n",
    "    height_df.dropna(subset=['GrPow_MW', 'avg_NH', 'avg_RD','plz_code'], inplace=True)\n",
    "\n",
    "\n",
    "    updated_plz_gdf['plz_code'] = updated_plz_gdf['plz_code'].astype(int)\n",
    "    height_df['plz_code'] = height_df['plz_code'].astype(int)\n",
    "    height_df['avg_NH'] = height_df['avg_NH'].astype(int)\n",
    "    # Merge dataframes on 'plz_code'\n",
    "    merged_df = pd.merge(updated_plz_gdf, height_df, on='plz_code', how='left')\n",
    "\n",
    "    # Replace NaN with 0 in 'avg_NH'\n",
    "    merged_df['avg_NH'].fillna(0, inplace=True)\n",
    "\n",
    "    # Create the 'hellmann' column\n",
    "    merged_df['hellmann'] = (merged_df['avg_NH'] / 10) ** 0.143\n",
    "\n",
    "    # Process wind speed columns\n",
    "    wind_speed_columns = merged_df.columns[merged_df.columns.str.startswith('wind_speed_mean_')]\n",
    "    merged_df[wind_speed_columns] = merged_df[wind_speed_columns].apply(pd.to_numeric, errors='coerce')\n",
    "    merged_df[wind_speed_columns] = merged_df[wind_speed_columns].multiply(merged_df['hellmann'], axis=0)\n",
    "\n",
    "    # Prepare final DataFrame\n",
    "    ws_height_df = merged_df[['plz_code', 'avg_NH', 'GrPow_MW', 'avg_RD', 'ID', 'Anzahl_Anlagen'] + list(wind_speed_columns)]\n",
    "\n",
    "    # Fill NaN values with 0 and replace inf/-inf with 0\n",
    "    ws_height_df.fillna(0, inplace=True)\n",
    "    ws_height_df.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "    # Convert columns to int\n",
    "    ws_height_df[['avg_NH', 'avg_RD', 'ID']] = ws_height_df[['avg_NH', 'avg_RD', 'ID']].astype(int)\n",
    "\n",
    "    print(\"Wind speed at hub height interpolated\")\n",
    "    # delete double entries\n",
    "    ws_height_df.drop_duplicates(subset='plz_code', keep='first', inplace=True)\n",
    "    # Save DataFrames to CSV\n",
    "    wind_speed_csv_path = f'{output_dir}/wind_speed_hub_height_{scenario}_{year_oi}.csv'\n",
    "    ws_height_df.to_csv(wind_speed_csv_path, sep=';')\n",
    "\n",
    "\n",
    "    \n",
    "    return ws_height_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ws_height_df=process_wind_speed_data(updated_plz_gdf, height_df, year_oistr, output_dir='Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weibull_probability_vectorized(shape, scale, bounds):\n",
    "    lower_bounds, upper_bounds = bounds[:-1], bounds[1:]\n",
    "    return weibull_min.cdf(upper_bounds, shape, scale=scale) - weibull_min.cdf(lower_bounds, shape, scale=scale)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_production(power_curves, year_oi,scenario, plz_shapefile):\n",
    "    from tqdm import tqdm\n",
    " \n",
    "    \"\"\"\n",
    "    Calculates the wind energy production based on Weibull parameters and wind speed data.\n",
    "\n",
    "    Parameters:\n",
    "    - ws_height_df: DataFrame containing wind speed data at hub height.\n",
    "    - weibull_params_df: DataFrame containing Weibull parameters by PLZ, year, and month.\n",
    "    - power_curves: DataFrame containing power curves data.\n",
    "    - year_oistr: Year of interest as a string.\n",
    "    - output_dir: Directory path where the output CSV file will be saved.\n",
    "\n",
    "    Returns:\n",
    "    - Total wind energy production in TWh for the specified year.\n",
    "    \"\"\"\n",
    "    year_oistr=str(year_oi)\n",
    "    ws_height_df=pd.read_csv(f'Data/wind_interim/wind_speed_hub_height_{scenario}_{year_oi}.csv',sep=';')\n",
    "    # Reshape ws_height_df to a long format for wind speeds\n",
    "    weibull_params_df= pd.read_csv(f'Data/Weibull/{scenario}/weibull_params_{year_oi}.csv',sep=',')\n",
    "    # Grouping by year, month, and windzone and calculating the mean of shape and scale\n",
    "    weibull_params_df = weibull_params_df.groupby(['year', 'month', 'windzone']).agg({'shape': 'mean', 'scale': 'mean'}).reset_index()\n",
    "\n",
    "    \n",
    "    plz_shapefile=plz_shapefile[['plz_code', 'windzone']]\n",
    "    \n",
    "    ws_height_df['plz_code']=ws_height_df['plz_code'].astype(int)\n",
    "    plz_shapefile['plz_code']=plz_shapefile['plz_code'].astype(int)\n",
    "    ws_height_df=pd.merge(ws_height_df,plz_shapefile, on='plz_code', how='left')\n",
    "    # Merge Weibull parameters\n",
    "\n",
    "   \n",
    "    \n",
    "    wind_speed_cols = [col for col in ws_height_df.columns if col.startswith('wind_speed_mean')]\n",
    "    \n",
    "    # Reshape ws_height_df to a long format for wind speeds\n",
    "    ws_height_long = ws_height_df.melt(id_vars=['ID', 'plz_code', 'windzone','avg_NH', 'GrPow_MW', 'avg_RD', 'Anzahl_Anlagen'], \n",
    "                                    value_vars=wind_speed_cols,\n",
    "                                    var_name='month', value_name='WindSpeedMean')\n",
    "\n",
    "    # Extract year and month\n",
    "    ws_height_long['year'] = ws_height_long['month'].str[-7:-3]\n",
    "    ws_height_long['month'] = ws_height_long['month'].str[-2:]\n",
    "    ws_height_long['year'] = ws_height_long['year'].astype(int)\n",
    "    ws_height_long['month'] = ws_height_long['month'].astype(int)\n",
    "\n",
    "    # Merge ws_height_long and weibull_params_df\n",
    "    merged_ws = pd.merge(ws_height_long, weibull_params_df, on=['month', 'year', 'windzone'], how='left')\n",
    "    merged_ws = merged_ws[merged_ws['Anzahl_Anlagen'] != 0]\n",
    "\n",
    "    # Prepare an empty DataFrame for storing probabilities\n",
    "    probabilities_df = pd.DataFrame()\n",
    "\n",
    "    # Define bounds for wind speeds\n",
    "    bounds = power_curves['WS_m/s'].to_numpy()\n",
    "    #scale_10m = mean_10m / np.exp(np.log(np.math.gamma(1 + 1/shape_10m)))\n",
    "    # Calculate probabilities for each wind speed bin for each row in merged_ws\n",
    "    for _, row in tqdm(merged_ws.iterrows(), total=merged_ws.shape[0], desc=\"Calculating Probabilities\"):\n",
    "        probabilities = weibull_probability_vectorized(row['shape']/2.5, row['scale'], bounds)\n",
    "        \n",
    "        # Prepare a DataFrame for the current row's probabilities\n",
    "        temp_df = pd.DataFrame({\n",
    "            'plz_code': row['plz_code'],\n",
    "            'year': row['year'],\n",
    "            'month': row['month'],\n",
    "            'WS_m/s': bounds[:-1],\n",
    "            'avg_RD': row['avg_RD'],\n",
    "            'Anzahl_Anlagen': row['Anzahl_Anlagen'],\n",
    "            'GrPow_MW': row['GrPow_MW'],\n",
    "            'Probability': probabilities\n",
    "        })\n",
    "        \n",
    "        # Append to the main probabilities DataFrame\n",
    "        probabilities_df = pd.concat([probabilities_df, temp_df], ignore_index=True)\n",
    "\n",
    "    # Save probabilities DataFrame to CSV\n",
    "    probabilities_df.to_csv(f'Data/wind_interimwind_speed_probabilities_{scenario}_{year_oi}.csv', index=False, sep=';')\n",
    "    # %%\n",
    "    # Reshaping power_curves to long format\n",
    "    power_curves_long = power_curves.melt(id_vars=['WS_m/s'], var_name='RD', value_name='Efficiency')\n",
    "\n",
    "    # integer\n",
    "    power_curves_long['RD'] = power_curves_long['RD'].astype(int)\n",
    "    probabilities_df['avg_RD'] = probabilities_df['avg_RD'].astype(int)\n",
    "\n",
    "\n",
    "    # Round avg_RD to the nearest multiple of 10 to match a power curve\n",
    "    probabilities_df['avg_RD'] = (probabilities_df['avg_RD'] / 10).round() * 10\n",
    "\n",
    "\n",
    "\n",
    "    # Merging probabilities_df with power_curves_long \n",
    "    probabilities_df = probabilities_df.merge(power_curves_long, left_on=['WS_m/s', 'avg_RD'], right_on=['WS_m/s', 'RD'], how='left')\n",
    "    #delete rows with NaN values\n",
    "    probabilities_df.dropna(subset=['Efficiency'], inplace=True)\n",
    "    \n",
    "    #drop the extra 'RD' column from the merge \n",
    "    probabilities_df.drop('RD', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # %%\n",
    "    #scatter plot of probabilities_df\n",
    "    import matplotlib.pyplot as plt\n",
    " \n",
    "\n",
    "    # %%\n",
    "    probabilities_df['Production']=probabilities_df['Probability']*probabilities_df['Efficiency']*probabilities_df['GrPow_MW']*720\n",
    "    #print(f'mean of probabilities:  {probabilities_df[\"Probability\"].mean()}')  \n",
    "    #sum of production column\n",
    "    tw=probabilities_df['Production'].sum()/1000000\n",
    "    tw=str(tw)\n",
    "    # Return total production\n",
    "    total_production_twh = probabilities_df['Production'].sum() / 1000000\n",
    "    print(f'Production in TWh: {total_production_twh} for year: {year_oi}')\n",
    "    # Save probabilities DataFrame to CSV\n",
    "    probabilities_df.to_csv(f'Data/wind_interimwind_speed_probabilities_{scenario}_{year_oi}.csv', index=False, sep=';')\n",
    "    return probabilities_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df32=pd.read_csv(f'Data/wind_interimwind_speed_probabilities_RCP26_2032.csv',  sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df=prob_df32\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your DataFrame 'df' is already loaded.\n",
    "# If it's in a CSV file, you can load it like this:\n",
    "# df = pd.read_csv('path_to_your_file.csv')\n",
    "\n",
    "# Grouping by 'WS_m/s' and summing up 'Probability'\n",
    "grouped_data = df.groupby('WS_m/s')['Production'].sum()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "grouped_data.plot(kind='bar', color='purple')  # Using bar plot for clearer visualization\n",
    "plt.title('Summed Production per WS_m/s')\n",
    "plt.xlabel('WS_m/s (Wind Speed in meter per second 2032)')\n",
    "plt.ylabel('Summed Production')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_df33['Testgroße']= prob_df33['GrPow_MW']*prob_df33['Probability'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df=prob_df33\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your DataFrame 'df' is already loaded.\n",
    "# If it's in a CSV file, you can load it like this:\n",
    "# df = pd.read_csv('path_to_your_file.csv')\n",
    "\n",
    "# Grouping by 'WS_m/s' and summing up 'Probability'\n",
    "grouped_data = df.groupby('WS_m/s')['Production'].sum()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "grouped_data.plot(kind='bar', color='purple')  # Using bar plot for clearer visualization\n",
    "plt.title('Summed Production per WS_m/s')\n",
    "plt.xlabel('WS_m/s (Wind Speed in meter per second 2033)')\n",
    "plt.ylabel('Summed Production')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your data\n",
    "# Assuming your DataFrame is named 'df' and is already loaded. \n",
    "# If it's in a CSV file, you can load it like this:\n",
    "# df = pd.read_csv('path_to_your_file.csv')\n",
    "df=prob_df34\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming your DataFrame 'df' is already loaded.\n",
    "# If it's in a CSV file, you can load it like this:\n",
    "# df = pd.read_csv('path_to_your_file.csv')\n",
    "\n",
    "# Grouping by 'WS_m/s' and summing up 'Probability'\n",
    "grouped_data = df.groupby('WS_m/s')['Production'].sum()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "grouped_data.plot(kind='bar', color='purple')  # Using bar plot for clearer visualization\n",
    "plt.title('Summed Production per WS_m/s')\n",
    "plt.xlabel('WS_m/s (Wind Speed in meter per second) 2034')\n",
    "plt.ylabel('Summed Production')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "# Load the basemap\n",
    "basemap_gdf = gpd.read_file('shp/georef-germany-postleitzahl/georef-germany-postleitzahl.shp')\n",
    "start_year = 2021\n",
    "end_year = 2050\n",
    "scenarios = ['RCP26', 'RCP45', 'RCP85']\n",
    "\n",
    "for scenario in scenarios:\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        base_path = 'Data/Results/'\n",
    "        file_namePV = f\"PV_power_output_{scenario}_{year}.csv\"\n",
    "        file_nameWI = f\"monthly_production_per_plz_{scenario}_{year}.csv\"\n",
    "        file_pathPV = os.path.join(base_path, file_namePV)\n",
    "        file_pathWI = os.path.join(base_path, file_nameWI)\n",
    "\n",
    "        if os.path.exists(file_pathPV) and os.path.exists(file_pathWI):\n",
    "            dataPV = pd.read_csv(file_pathPV, sep=',')\n",
    "            dataPV.drop_duplicates(subset=['plz_code', 'month'], inplace=True)\n",
    "            dataPV['Production_PV'] = dataPV['Production'] / 1000  # Convert kW to MW\n",
    "            \n",
    "            dataWI = pd.read_csv(file_pathWI, sep=';')\n",
    "            dataWI.drop_duplicates(subset=['plz_code', 'month'], inplace=True)\n",
    "            dataWI.rename(columns={'Production': 'Production_wind'}, inplace=True)\n",
    "            \n",
    "            basemap_gdf['plz_code'] = basemap_gdf['plz_code'].astype(int)\n",
    "\n",
    "            for month in range(1, 13):\n",
    "                monthly_dataPV = dataPV[dataPV['month'] == month]\n",
    "                monthly_dataWI = dataWI[dataWI['month'] == month]\n",
    "                \n",
    "                # Merge the data\n",
    "                # Merge basemap_gdf with monthly_dataPV, filling missing production values with 1\n",
    "                merged_gdf = basemap_gdf.merge(monthly_dataPV[['plz_code', 'Production_PV']], on='plz_code', how='left').fillna({'Production_PV': 1})\n",
    "\n",
    "                merged_gdf = merged_gdf.merge(monthly_dataWI[['plz_code', 'Production_wind']], on='plz_code', how='left')\n",
    "                merged_gdf['Total_Production'] = merged_gdf['Production_wind'].fillna(1) + merged_gdf['Production_PV'].fillna(1)\n",
    "\n",
    "                # Fill NaNs and zeros with 1 for log scale plotting purposes\n",
    "                merged_gdf['Production_wind'].replace(0, 1, inplace=True)\n",
    "                merged_gdf['Production_wind'].fillna(1, inplace=True)\n",
    "\n",
    "                # Plotting individual and combined production maps\n",
    "                # PV Production Plot\n",
    "                fig, ax = plt.subplots(1, figsize=(15, 10))\n",
    "                merged_gdf.plot(column='Production_PV', ax=ax, legend=True,\n",
    "                                norm=LogNorm(vmin=1, vmax=750000),\n",
    "                                legend_kwds={'label': \"PV Production by PLZ\", 'orientation': \"horizontal\"},\n",
    "                                cmap='viridis')\n",
    "                plt.title(f'PV Production for {scenario} - {year}/{month}')\n",
    "                plt.savefig(f'Data/Results/Maps/{scenario}/PV_{scenario}_{year}_month_{month}.png')\n",
    "                plt.close()\n",
    "\n",
    "                # Wind Production Plot\n",
    "                fig, ax = plt.subplots(1, figsize=(15, 10))\n",
    "                merged_gdf.plot(column='Production_wind', ax=ax, legend=True,\n",
    "                                norm=LogNorm(vmin=1, vmax=750000),\n",
    "                                legend_kwds={'label': \"Wind Production by PLZ\", 'orientation': \"horizontal\"},\n",
    "                                cmap='viridis')\n",
    "                plt.title(f'Wind Production for {scenario} - {year}/{month}')\n",
    "                plt.savefig(f'Data/Results/Maps/{scenario}/Wind_{scenario}_{year}_month_{month}.png')\n",
    "                plt.close()\n",
    "\n",
    "\n",
    "                # Combined Production Plot\n",
    "                fig, ax = plt.subplots(1, figsize=(15, 10))\n",
    "                merged_gdf.plot(column='Total_Production', ax=ax, legend=True,\n",
    "                                norm=LogNorm(vmin=1, vmax=750000),\n",
    "                                legend_kwds={'label': \"Total Production by PLZ\", 'orientation': \"horizontal\"},\n",
    "                                cmap='viridis')\n",
    "                plt.savefig(f'Data/Results/Maps/{scenario}/Joined_{scenario}_{year}_{month}.png')\n",
    "                plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def monthly_production_save(year, scenario):\n",
    "    probabilities_df=pd.read_csv(f'Data/wind_interimwind_speed_probabilities_{scenario}_{year}.csv', sep=';')\n",
    "\n",
    "\n",
    "    # Assuming df is your DataFrame\n",
    "    aggregated_df = probabilities_df.groupby(['plz_code', 'month']).agg({\n",
    "        'WS_m/s': 'first',\n",
    "        'avg_RD': 'first',\n",
    "        'Anzahl_Anlagen': 'first',\n",
    "        'GrPow_MW': 'first',\n",
    "        'Probability': 'mean',\n",
    "        'Efficiency': 'mean',\n",
    "        'Production': 'sum'\n",
    "    }).reset_index()\n",
    "    aggregated_df\n",
    "\n",
    "    plz_zerros=plz_shapefile['plz_code']\n",
    "    plz_zerros=plz_zerros.astype(int)\n",
    "    plz_zerros=pd.DataFrame(plz_zerros)\n",
    "    plz_zerros\n",
    "    #concatenate the plz_codes with the aggregated_df\n",
    "    # Sample data setup (replace this with your actual DataFrame loading)\n",
    "    # Assume 'aggregated_df' and 'plz_code_df' are already loaded as described\n",
    "\n",
    "    # Check which PLZ codes are missing in the aggregated DataFrame\n",
    "    missing_plz = plz_zerros[~plz_zerros['plz_code'].isin(aggregated_df['plz_code'])]\n",
    "\n",
    "    # Create a DataFrame for missing PLZ codes with default values\n",
    "    default_data = {\n",
    "        'WS_m/s': 0,\n",
    "        'avg_RD': 0,\n",
    "        'Anzahl_Anlagen': 0,\n",
    "        'GrPow_MW': 0,\n",
    "        'Probability': 0,\n",
    "        'Efficiency': 0,\n",
    "        'Production': 0\n",
    "    }\n",
    "    months = list(range(1, 13))  # Months from 1 to 12\n",
    "\n",
    "    # Using a list comprehension to build the DataFrame\n",
    "    missing_rows = pd.DataFrame([\n",
    "        {**{'plz_code': plz, 'month': month}, **default_data}\n",
    "        for plz in missing_plz['plz_code'] for month in months\n",
    "    ])\n",
    "\n",
    "    # Concatenate the new rows with the original aggregated DataFrame\n",
    "    final_df = pd.concat([aggregated_df, missing_rows], ignore_index=True)\n",
    "    final_df.to_csv(f'Data/Results/monthly_production_per_plz_{scenario}_{year}.csv', index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_and_save_production(scenario, year_oi, output_dir='Data/Results'):\n",
    "    \"\"\"\n",
    "    adds the production of areas without installations (0 values) for depiction later saves the results along with monthly production to CSV files.\n",
    "\n",
    "    Parameters:\n",
    "    - updated_plz_gdf: GeoDataFrame with updated PLZ data.\n",
    "    - probabilities_df: DataFrame with probabilities and production data.\n",
    "    - ws_height_df: DataFrame with wind speed data at hub height.\n",
    "    - year_oistr: Year of interest as a string.\n",
    "    - output_dir: Directory path where the output CSV files will be saved.\n",
    "    \"\"\"\n",
    "    updated_plz_gdf=pd.read_csv(f'Data/wind_interimupdated_plz_{scenario}_wind_speed_mean_{year_oi}.csv')\n",
    "    probabilities_df=pd.read_csv(f'Data/wind_interimwind_speed_probabilities_{scenario}_{year_oi}.csv', sep=';')\n",
    "    ws_height_df=pd.read_csv(f'Data/wind_interimwind_speed_hub_height_{scenario}_{year_oi}.csv', sep=';')\n",
    "    print(ws_height_df.columns)  # This will show the exact column names as pandas sees them\n",
    "    plz_codes = updated_plz_gdf['plz_code'].unique()\n",
    "    plz_codes = pd.DataFrame(plz_codes, columns=['plz_code'])\n",
    "    plz_codes = plz_codes.astype(int)\n",
    "    \n",
    "    months = range(1, 13)  # Months 1 through 12\n",
    "    wind_speeds = np.arange(0, 25, 0.5)  # Wind speeds from 0 to 24.5 in 0.5 steps\n",
    "    \n",
    "    # Generate all combinations including wind speeds\n",
    "\n",
    "    all_combinations_extended = pd.MultiIndex.from_product(\n",
    "        [plz_codes['plz_code'], [year_oi], months, wind_speeds],  # Use plz_codes['plz_code'] here\n",
    "        names=['plz_code', 'year', 'month', 'WS_m/s']\n",
    "    ).to_frame(index=False)\n",
    "\n",
    "    # Merge to include wind speed combinations and fill missing values\n",
    "    probabilities_extended = pd.merge(\n",
    "        all_combinations_extended, probabilities_df,\n",
    "        on=['plz_code', 'year', 'month', 'WS_m/s'],\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "    # Fill missing values\n",
    "    columns_to_fill = ['avg_RD', 'Anzahl_Anlagen', 'GrPow_MW', 'Probability', 'Efficiency', 'Production']\n",
    "    probabilities_extended[columns_to_fill] = probabilities_extended[columns_to_fill].fillna(0)\n",
    "\n",
    "    # Aggregate production per PLZ and merge with additional data\n",
    "    production_per_plz = probabilities_extended.groupby('plz_code')['Production'].sum().reset_index()\n",
    "    production_per_plz = pd.merge(production_per_plz, ws_height_df[['plz_code', 'Anzahl_Anlagen', 'avg_RD', 'GrPow_MW', 'avg_NH']], on='plz_code', how='right')\n",
    "\n",
    "    # Save aggregated production per PLZ\n",
    "    production_per_plz.to_csv(f'{output_dir}/production_per_plz{scenario}_{year_oi}.csv', index=False)\n",
    "\n",
    "    # Aggregate and save monthly production\n",
    "    file_path = f'{output_dir}/monthly_production_per_plz_{scenario}_{year_oi}.csv'\n",
    "    if os.path.exists(file_path):\n",
    "        existing_data = pd.read_csv(file_path)\n",
    "    else:\n",
    "        existing_data = pd.DataFrame(columns=['plz_code', 'year', 'month', 'Production'])\n",
    "\n",
    "    new_data = probabilities_df.groupby(['plz_code', 'year', 'month'])['Production'].sum().reset_index()\n",
    "    updated_data = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "    updated_data.to_csv(file_path, index=False)\n",
    "\n",
    "    print(f'Results saved for year: {year_oi}')\n",
    "    return production_per_plz\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#production_per_plz= calculate_and_save_production(updated_plz_gdf, probabilities_df, ws_height_df, year_oistr, year_oi, output_dir='Data/Results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define RCP scenarios and years\n",
    "import datetime\n",
    "start_year = 2021\n",
    "end_year = 2050\n",
    "rcp_scenarios = {\n",
    "    'RCP26': 'get_data/sfcWind_EUR-11_MPI-M-MPI-ESM-LR_rcp26_r1i1p1_SMHI-RCA4_v1a_mon_202101-205012.nc',\n",
    "    'RCP45': 'get_data/sfcWind_EUR-11_MPI-M-MPI-ESM-LR_rcp45_r1i1p1_SMHI-RCA4_v1a_mon_202101-205012.nc',\n",
    "    'RCP85': 'get_data/sfcWind_EUR-11_MPI-M-MPI-ESM-LR_rcp85_r1i1p1_SMHI-RCA4_v1a_mon_202101-205012.nc',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loop through each RCP scenario\n",
    "for rcp, rcp_path in rcp_scenarios.items():\n",
    "    print(f\"[{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}]:Processing for RCP scenario: {rcp}\")\n",
    "    ds=xr.open_dataset(rcp_path)\n",
    "    germany_gdf=create_germany_gdf_from_ds(ds)\n",
    "    print(\"germany_gdf erzeugt\")\n",
    "    processed_data = process_year_for_germany( germany_gdf_template, paths)\n",
    "    updated_plz_gdf = process_year(germany_gdf, rcp)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mastr='Data//2023_MASTR_ALR.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for all years, 2023=Start year, 2050=end year  \n",
    "prepare_height_df(2023, input_dir='Data/', output_dir='Data/Wind_interim')\n",
    "calculate_annual_contraction(2023, 2050)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 2023\n",
    "end_year = 2050\n",
    "for year in range(start_year, end_year + 1):\n",
    "    print(f\"Year: {year}\")\n",
    "    year_oi=year\n",
    "    year_oistr=str(year_oi)\n",
    "    year_befoistr=str(year_oi-1)\n",
    "    expansion_chunk_value=0.2237*year-448.08\n",
    "   #height_df=prepare_height_df(year_befoistr, year_oi, input_dir='Data/', output_dir='Data/Wind_interim')\n",
    "    height_df=distribute_expansion_randomly(year, expansion_chunk_value)\n",
    "    #print(f\"[{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}]: Data preparation for year {year} completed.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# start_year = 2024\n",
    "# end_year = 2049\n",
    "scenarios = ['RCP26', 'RCP45', 'RCP85']\n",
    "\n",
    "# for scenario in scenarios:\n",
    "#     print(f\"[{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}]: Processing for scenario {scenario}\")\n",
    "#     for year in range(start_year, end_year + 1):\n",
    "#         ws_height_df = pd.read_csv(f'Data/wind_interimwind_speed_hub_height_{scenario}_{year}.csv', sep=';')\n",
    "#         wind_speed_columns = [col for col in ws_height_df.columns if col.startswith('wind_speed_mean')]\n",
    "#         # ws_height_df['weibull_params'] = ws_height_df.apply(lambda row: fitweibull(row[wind_speed_columns].values), axis=1)\n",
    "#         # ws_height_df[['weibull_shape', 'weibull_scale']] = pd.DataFrame(ws_height_df['weibull_params'].tolist(), index=ws_height_df.index)\n",
    "#         # # Extract only necessary columns for saving\n",
    "#         # save_df = ws_height_df[['plz_code', 'weibull_shape', 'weibull_scale']]\n",
    "#         # Save to CSV\n",
    "#         output_filename = f\"Data/Weibull/weibull_params_per_month_{scenario}_{year}.csv\"\n",
    "#         save_df.to_csv(output_filename, index=False)\n",
    "#         print(f\"Saved Weibull parameters to {output_filename}\")\n",
    "#         # Optionally, print the DataFrame to verify results\n",
    "#         print(save_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 2031\n",
    "end_year = 2050\n",
    "for rcp, rcp_path in rcp_scenarios.items():\n",
    "    print(f\"Final processing for RCP scenario: {rcp}\")\n",
    "    \n",
    "    for year in range(start_year, end_year + 1):\n",
    "        print(f\"Final processing for year: {year}\")\n",
    "        year_oi=year\n",
    "        year_oistr=str(year_oi)\n",
    "        year_befoistr=str(year_oi-1)\n",
    "        \n",
    "        # Load or access ws_height_df and other necessary data produced in the first loop\n",
    "        #weibull_params=pd.read_csv(f'Data/Weibull/weibull_params_per_month_{rcp}_{year_oi}.csv', sep=',')\n",
    "        #weibull_params=prepare_Weibull(weibull_params, year_oi, rcp)\n",
    "        \n",
    "        #add a column with year and month. all year values are year, and month is from 1 to 12\n",
    "\n",
    "        #probabilities_df = calculate_production(power_curves, year_oistr, output_dir='Data/Wind_interim')\n",
    "        # generate_weibull_params(ws_height_df,plz_shapefile)\n",
    "        #ws_height_df=process_wind_speed_data(rcp, year_oi, year_oistr, output_dir='Data/Wind_interim')\n",
    "        # generate_weibull_params(ws_height_df,plz_shapefile)\n",
    "        probabilities_df=calculate_production(power_curves, year_oi, rcp, plz_shapefile)\n",
    "        production_per_plz = add_and_save_production(rcp,year_oi, output_dir='Data/Results') #deprecated\n",
    "        monthly_production_save(year, rcp)\n",
    "\n",
    "        print(f\"[{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}]: Final analysis for year {year} and RCP scenario {rcp} completed.\")\n",
    "    print(f\"[{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}]: Final analysis for RCP scenario {rcp} completed.\")\n",
    "print(\"All processing completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "# Load the basemap\n",
    "basemap_gdf = gpd.read_file('shp/georef-germany-postleitzahl/georef-germany-postleitzahl.shp')\n",
    "start_year = 2023\n",
    "end_year = 2050\n",
    "scenarios = [ 'RCP85']\n",
    "\n",
    "for scenario in scenarios:\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        base_path = 'Data/Results/'\n",
    "        file_namePV = f\"PV_power_output_{scenario}_{year}.csv\"\n",
    "        file_nameWI = f\"monthly_production_per_plz_{scenario}_{year}.csv\"\n",
    "        file_pathPV = os.path.join(base_path, file_namePV)\n",
    "        file_pathWI = os.path.join(base_path, file_nameWI)\n",
    "\n",
    "        if os.path.exists(file_pathPV) and os.path.exists(file_pathWI):\n",
    "            dataPV = pd.read_csv(file_pathPV, sep=',')\n",
    "            dataPV.drop_duplicates(subset=['plz_code', 'month'], inplace=True)\n",
    "            dataPV['Production_PV'] = dataPV['Production'] / 1000  # Convert kW to MW\n",
    "            \n",
    "            dataWI = pd.read_csv(file_pathWI, sep=';')\n",
    "            dataWI.drop_duplicates(subset=['plz_code', 'month'], inplace=True)\n",
    "            dataWI.rename(columns={'Production': 'Production_wind'}, inplace=True)\n",
    "            \n",
    "            basemap_gdf['plz_code'] = basemap_gdf['plz_code'].astype(int)\n",
    "\n",
    "            for month in range(1, 13):\n",
    "                monthly_dataPV = dataPV[dataPV['month'] == month]\n",
    "                monthly_dataWI = dataWI[dataWI['month'] == month]\n",
    "                \n",
    "                # Merge the data\n",
    "                # Merge basemap_gdf with monthly_dataPV, filling missing production values with 1\n",
    "                merged_gdf = basemap_gdf.merge(monthly_dataPV[['plz_code', 'Production_PV']], on='plz_code', how='left').fillna({'Production_PV': 1})\n",
    "\n",
    "                merged_gdf = merged_gdf.merge(monthly_dataWI[['plz_code', 'Production_wind']], on='plz_code', how='left')\n",
    "                merged_gdf['Total_Production'] = merged_gdf['Production_wind'].fillna(1) + merged_gdf['Production_PV'].fillna(1)\n",
    "\n",
    "                # Fill NaNs and zeros with 1 for log scale plotting purposes\n",
    "                merged_gdf['Production_wind'].replace(0, 1, inplace=True)\n",
    "                merged_gdf['Production_wind'].fillna(1, inplace=True)\n",
    "\n",
    "                # Plotting individual and combined production maps\n",
    "                # PV Production Plot\n",
    "                fig, ax = plt.subplots(1, figsize=(15, 10))\n",
    "                merged_gdf.plot(column='Production_PV', ax=ax, legend=True,\n",
    "                                norm=LogNorm(vmin=1, vmax=750000),\n",
    "                                legend_kwds={'label': \"PV Production by PLZ\", 'orientation': \"horizontal\"},\n",
    "                                cmap='viridis')\n",
    "                plt.title(f'PV Production for {scenario} - {year}/{month}')\n",
    "                plt.savefig(f'Data/Results/Maps/{scenario}/PV_{scenario}_{year}_month_{month}.png')\n",
    "                plt.close()\n",
    "\n",
    "                # Wind Production Plot\n",
    "                fig, ax = plt.subplots(1, figsize=(15, 10))\n",
    "                merged_gdf.plot(column='Production_wind', ax=ax, legend=True,\n",
    "                                norm=LogNorm(vmin=1, vmax=750000),\n",
    "                                legend_kwds={'label': \"Wind Production by PLZ\", 'orientation': \"horizontal\"},\n",
    "                                cmap='viridis')\n",
    "                plt.title(f'Wind Production for {scenario} - {year}/{month}')\n",
    "                plt.savefig(f'Data/Results/Maps/{scenario}/Wind_{scenario}_{year}_month_{month}.png')\n",
    "                plt.close()\n",
    "\n",
    "\n",
    "                # Combined Production Plot\n",
    "                fig, ax = plt.subplots(1, figsize=(15, 10))\n",
    "                merged_gdf.plot(column='Total_Production', ax=ax, legend=True,\n",
    "                                norm=LogNorm(vmin=1, vmax=750000),\n",
    "                                legend_kwds={'label': \"Total Production by PLZ\", 'orientation': \"horizontal\"},\n",
    "                                cmap='viridis')\n",
    "                plt.savefig(f'Data/Results/Maps/{scenario}/Joined_{scenario}_{year}_{month}.png')\n",
    "                plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the directory containing the CSV files\n",
    "directory = 'Data/wind_interim'\n",
    "\n",
    "# Initialize a list to store the data\n",
    "data = []\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith('wind_speed_hub_height') and filename.endswith('.csv'):\n",
    "        # Construct the full path to the file\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Extract the scenario and year from the filename   \n",
    "        parts = filename.split('_')\n",
    "        scenario = parts[4]  # Assumes 'RCP45' is always in the same position\n",
    "        year = parts[5].split('.')[0]  # Split on '.' to remove the file extension\n",
    "        \n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(file_path, sep=';')\n",
    "        \n",
    "        # Check if 'GrPow_MW' column exists in the dataframe\n",
    "        if 'GrPow_MW' in df.columns:\n",
    "            # Filter out rows where 'GrPow_MW' is not 0\n",
    "            #df_filtered = df[df['GrPow_MW'] != 0]\n",
    "            \n",
    "            # Calculate the mean of 'avg_RD' column\n",
    "            GrPow_MW = df['GrPow_MW'].sum()\n",
    "            \n",
    "            # Append the mean, scenario, and year to the list\n",
    "            data.append({'GrPow_MW': GrPow_MW, 'Scenario': scenario, 'Year': year})\n",
    "\n",
    "# Convert the list to a DataFrame outside the loop\n",
    "final_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('GrPow_MW_summary.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter for scenario RCP26\n",
    "rcp26_data = final_df[final_df['Scenario'] == 'RCP26']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 10))  # Set the figure size (optional)\n",
    "plt.plot(rcp26_data['Year'], rcp26_data['GrPow_MW'], marker='o', linestyle='-')\n",
    "plt.title('GrPow_MW for Scenario RCP26 Over the Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('GrPow_MW')\n",
    "plt.grid(True)\n",
    "plt.xticks(rcp26_data['Year'])  # Ensure all years are marked\n",
    "plt.tight_layout()  # Adjust layout to not cut off labels\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the directory containing the CSV files\n",
    "directory = 'Data/Weibull/RCP26/'\n",
    "\n",
    "# Initialize a list to store the data\n",
    "data = []\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith('weibull_params_') and filename.endswith('.csv'):\n",
    "        # Construct the full path to the file\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Extract the scenario and year from the filename   \n",
    "        parts = filename.split('_')\n",
    "        #scenario = parts[4]  # Assumes 'RCP45' is always in the same position\n",
    "        year = parts[2].split('.')[0]  # Split on '.' to remove the file extension\n",
    "        \n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(file_path, sep=',')\n",
    "        \n",
    "        # Check if 'GrPow_MW' column exists in the dataframe\n",
    "        if 'shape' in df.columns:\n",
    "            # Filter out rows where 'GrPow_MW' is not 0\n",
    "            #df_filtered = df[df['GrPow_MW'] != 0]\n",
    "            \n",
    "            # Calculate the mean of 'avg_RD' column\n",
    "            scale = df['shape'].mean()\n",
    "            \n",
    "            # Append the mean, scenario, and year to the list\n",
    "            data.append({'shapemean': scale,  'Year': year})\n",
    "\n",
    "# Convert the list to a DataFrame outside the loop\n",
    "final_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# Specify the directory containing the CSV files\n",
    "directory = 'Data/Results/'\n",
    "\n",
    "# Initialize a list to store the data\n",
    "data = []\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith('monthly_production_per_plz_') and filename.endswith('.csv'):\n",
    "        # Construct the full path to the file\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Extract the scenario and year from the filename   \n",
    "        parts = filename.split('_')\n",
    "        scenario = parts[4]  # Assumes 'RCP45' is always in the same position\n",
    "        year = parts[5].split('.')[0]  # Split on '.' to remove the file extension\n",
    "        \n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(file_path, sep=';')\n",
    "        \n",
    "        # Check if 'GrPow_MW' column exists in the dataframe\n",
    "        if 'Production' in df.columns:\n",
    "            # Sum the 'GrPow_MW' column\n",
    "            Production = df['Production'].sum()\n",
    "            \n",
    "            # Append the sum, scenario, and year to the list\n",
    "            data.append({'Production': Production, 'Scenario': scenario, 'Year': year})\n",
    "\n",
    "# Convert the list to a DataFrame outside the loop\n",
    "final_df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Filter for scenario RCP26\n",
    "rcp26_data = final_df[final_df['Scenario'] == 'RCP85']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 10))  # Set the figure size (optional)\n",
    "plt.plot(rcp26_data['Year'], rcp26_data['Production'], marker='o', linestyle='-')\n",
    "plt.title('Production Sum for Scenario RCP85 Over the Years', fontsize=32)\n",
    "plt.xlabel('Year', fontsize=22)\n",
    "plt.ylabel('Production Sum (TWh)', fontsize=22)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.xticks(rcp26_data['Year'], fontsize= 11)  # Ensure all years are marked\n",
    "plt.tight_layout()  # Adjust layout to not cut off labels\n",
    "plt.savefig('Literatur/Grafiken/Wind_Production_Sum_RCP85.png')\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# Specify the directory containing the CSV files\n",
    "directory = 'Data/Results/'\n",
    "\n",
    "# Initialize a list to store the data\n",
    "data = []\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith('PV_power_output_') and filename.endswith('.csv'):\n",
    "        # Construct the full path to the file\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Extract the scenario and year from the filename   \n",
    "        parts = filename.split('_')\n",
    "        scenario = parts[3]  # Assumes 'RCP45' is always in the same position\n",
    "        year = parts[4].split('.')[0]  # Split on '.' to remove the file extension\n",
    "        \n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(file_path, sep=',')\n",
    "        \n",
    "        # Check if 'GrPow_MW' column exists in the dataframe\n",
    "        if 'Production' in df.columns:\n",
    "            # Sum the 'GrPow_MW' column\n",
    "            Production = df['Production'].sum()\n",
    "            \n",
    "            # Append the sum, scenario, and year to the list\n",
    "            data.append({'Production': Production, 'Scenario': scenario, 'Year': year})\n",
    "\n",
    "# Convert the list to a DataFrame outside the loop\n",
    "final_df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Filter for scenario RCP26\n",
    "rcp26_data = final_df[final_df['Scenario'] == 'RCP45']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 10))  # Set the figure size (optional)\n",
    "plt.plot(rcp26_data['Year'], rcp26_data['Production'], marker='o', linestyle='-')\n",
    "plt.title('PV Production Sum for Scenario RCP45 Over the Years', fontsize=32)\n",
    "plt.xlabel('Year', fontsize=22)\n",
    "plt.ylabel('Production Sum (TWh)', fontsize=22)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.xticks(rcp26_data['Year'], fontsize= 11)  # Ensure all years are marked\n",
    "plt.tight_layout()  # Adjust layout to not cut off labels\n",
    "plt.savefig('Literatur/Grafiken/PV_Production_Sum_RCP45.png')\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Specify the directory containing the CSV files\n",
    "directory = 'Data/wind_interim'\n",
    "\n",
    "# Initialize a list to store the data\n",
    "data = []\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith('wind_speed_probabilities_') and filename.endswith('.csv'):\n",
    "        # Construct the full path to the file\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Extract the scenario and year from the filename   \n",
    "        parts = filename.split('_')\n",
    "        scenario = parts[3]  # Assumes 'RCP45' is always in the same position\n",
    "        year = parts[4].split('.')[0]  # Split on '.' to remove the file extension\n",
    "        \n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(file_path, sep=';')\n",
    "        \n",
    "        # Check if 'GrPow_MW' column exists in the dataframe\n",
    "        if 'Probability' in df.columns:\n",
    "            # Sum the 'GrPow_MW' column\n",
    "            Probability = df['Probability'].mean()\n",
    "            \n",
    "            # Append the sum, scenario, and year to the list\n",
    "            data.append({'Probability': Probability, 'Scenario': scenario, 'Year': year})\n",
    "\n",
    "# Convert the list to a DataFrame outside the loop\n",
    "final_df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "final_df\n",
    "# Filter for scenario RCP26\n",
    "rcp26_data = final_df[final_df['Scenario'] == 'RCP26']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 10))  # Set the figure size (optional)\n",
    "plt.plot(rcp26_data['Year'], rcp26_data['Probability'], marker='o', linestyle='-')\n",
    "plt.title('Probability sum in monthly Production for Scenario RCP26 Over the Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Probability')\n",
    "plt.grid(True)\n",
    "plt.xticks(rcp26_data['Year'])  # Ensure all years are marked\n",
    "plt.tight_layout()  # Adjust layout to not cut off labels\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Data/wind_interim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Specify the directory containing the CSV files\n",
    "directory = 'Data/wind_interim'\n",
    "\n",
    "# Initialize a list to store the data\n",
    "data = []\n",
    "\n",
    "# Loop through each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith('updated_plz_') and filename.endswith('.csv'):\n",
    "        # Construct the full path to the file\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Extract the scenario and year from the filename   \n",
    "        parts = filename.split('_')\n",
    "        scenario = parts[2]  # Assumes 'RCP45' is always in the same position\n",
    "        year = parts[6].split('.')[0]  # Split on '.' to remove the file extension\n",
    "        \n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(file_path, sep=';')\n",
    "        \n",
    "        # Check if 'GrPow_MW' column exists in the dataframe\n",
    "        if 'Probability' in df.columns:\n",
    "            # Sum the 'GrPow_MW' column\n",
    "            Probability = df['Probability'].mean()\n",
    "            \n",
    "            # Append the sum, scenario, and year to the list\n",
    "            data.append({'Probability': Probability, 'Scenario': scenario, 'Year': year})\n",
    "\n",
    "# Convert the list to a DataFrame outside the loop\n",
    "final_df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "final_df\n",
    "# Filter for scenario RCP26\n",
    "rcp26_data = final_df[final_df['Scenario'] == 'RCP26']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 10))  # Set the figure size (optional)\n",
    "plt.plot(rcp26_data['Year'], rcp26_data['Probability'], marker='o', linestyle='-')\n",
    "plt.title('Probability sum in monthly Production for Scenario RCP26 Over the Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Probability')\n",
    "plt.grid(True)\n",
    "plt.xticks(rcp26_data['Year'])  # Ensure all years are marked\n",
    "plt.tight_layout()  # Adjust layout to not cut off labels\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcp26_data\n",
    "rcp26_data['Yearly_Change'] = rcp26_data['GrPow_MW_Sum'].diff()\n",
    "\n",
    "# Display the DataFrame to see the new column\n",
    "print(rcp26_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 20\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(file_path):\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Process data for each month\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m month \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m13\u001b[39m):  \u001b[38;5;66;03m# Months from January to December\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m         monthly_data \u001b[38;5;241m=\u001b[39m data[\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmonth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m month]\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;66;03m# Merge with shapefile data\u001b[39;00m\n\u001b[0;32m     23\u001b[0m         merged_gdf \u001b[38;5;241m=\u001b[39m basemap_gdf\u001b[38;5;241m.\u001b[39mmerge(monthly_data, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m, left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplz_code\u001b[39m\u001b[38;5;124m'\u001b[39m, right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplz_code\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "basemap_gdf = gpd.read_file('shp/georef-germany-postleitzahl/georef-germany-postleitzahl.shp')\n",
    "start_year = 2023\n",
    "end_year = 2023\n",
    "\n",
    "scenarios = ['RCP26', 'RCP45', 'RCP85']\n",
    "\n",
    "for scenario in scenarios:\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        base_path='Data/Results/'\n",
    "        file_name = f\"monthly_production_per_plz_{scenario}_{year}.csv\"\n",
    "        file_path = os.path.join(base_path, file_name)\n",
    "        basemap_gdf['plz_code'] = basemap_gdf['plz_code'].astype(int)\n",
    "    \n",
    "\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(file_path):\n",
    "\n",
    "            # Process data for each month\n",
    "            for month in range(1, 13):  # Months from January to December\n",
    "                monthly_data = data[data['month'] == month]\n",
    "                \n",
    "                # Merge with shapefile data\n",
    "                merged_gdf = basemap_gdf.merge(monthly_data, how='left', left_on='plz_code', right_on='plz_code')\n",
    "                \n",
    "                # Plotting\n",
    "                fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
    "                merged_gdf.plot(column='Production', ax=ax, legend=True,\n",
    "                                legend_kwds={'label': \"Production by PLZ\",\n",
    "                                             'orientation': \"horizontal\"})\n",
    "                plt.title(f'Energy Production for {scenario} - {year}/{month}')\n",
    "                #plt.savefig(f'Data/Results/Maps/{scenario}/{scenario}_{year}_month_{month}.png')  # Save the figure\n",
    "                plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
